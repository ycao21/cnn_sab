%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

% \documentclass[sigconf]{acmart}
\documentclass[sigconf,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}

% \documentclass[sigconf]{acmart}

\copyrightyear{2026}
\acmYear{2026}
\setcopyright{cc}
\setcctype{by}
\acmConference[SIGIR '26]{Proceedings of the ACM Web Conference 2026}{April 13--17, 2026}{Dubai, United Arab Emirates}
% \acmBooktitle{Proceedings of}
\acmPrice{}
\acmDOI{xxxxxxxxxxx}
\acmISBN{xxxxxxxxxxxx}

% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.



\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}

% % COMMENTS
\newif\ifcomment
\commenttrue % comment out to hide comments
\providecommand{\YC}[1]{\ifcomment{\small \color{blue} [YC: #1]}\fi} %Yanan
\providecommand{\AR}[1]{\ifcomment{\small \color{red} [AR: #1]}\fi} %Ashish
% \providecommand{\KZ}[1]{\ifcomment{\small \color{teal} [KZ: #1]}\fi} %
% \providecommand{\FF}[1]{\ifcomment{\small \color{olive} [FF: #1]}\fi} %
% \providecommand{\LMo}[1]{\ifcomment{\small \color{orange} [LMo: #1]}\fi} %
% \providecommand{\MD}[1]{\ifcomment{\small \color{cyan} [MD: #1]}\fi} %


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{CASE: Cadence-Aware Set Encoding for Large-Scale Next Basket Repurchase Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Anonymous Author(s)}
% \date{}
% \settopmatter{authorsperrow=4}

\author{Yanan Cao}
\authornote{Highlighted authors contributed equally to this research.}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{yanan.cao@walmart.com}

\author{Ashish Ranjan}
\authornotemark[1]
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ashish.ranjan0@walmart.com}

\author{Sinduja Subramaniam}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{sinduja.subramaniam@walmart.com}

\author{Evren Korpeoglu}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ekorpeoglu@walmart.com}

\author{Kaushiki Nag}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kaushiki.nag@walmart.com}

\author{Kannan Achan}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kannan.achan@walmart.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Author et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Repurchase prediction is the dominant signal in large-scale retail recommendation: most items in a user's next basket have been purchased before, and their timing follows a stable personal cadence. Yet most next basket recommendation (NBR) models frame the problem as a sequence of discrete basket events indexed by visit count, which cannot represent elapsed calendar time or adapt scores between transactions. Models that do capture calendar-time cadence rely on per-user hypernetworks to generate personalized filters, incurring parameterization costs that prohibit deployment at scale. We present CASE (Cadence-Aware Set Encoding), which decouples item-level temporal cadence encoding from cross-item set modeling. CASE applies shared multi-scale convolutional filters over a fixed calendar horizon to capture recurring repurchase rhythms, and uses induced set attention to model cross-item dependencies --- without per-user parameters. Evaluated against a deployed production system on over 10 million users and 500K items, CASE achieves up to 14\% relative Recall lift at $K{=}1$ and 8\% at $K{=}10$, demonstrating that scalable cadence-aware modeling yields measurable gains in industrial NBR.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Temporal reasoning}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Cadence Modeling, Next Basket Recommendation, Sequential Modeling}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\linewidth]{figure1.png}
    \caption{diagram}
    \label{fig:diagram}
\end{figure*}

In large-scale grocery and retail platforms, the majority of items in a user’s next basket have been purchased before. Repurchase timing follows item-specific cadence --- staples weekly, household goods monthly --- and accurate timing directly drives customer experience: recommend too early and suggestions feel irrelevant; too late and the item may already be in the cart from a competing channel. This makes next basket repurchase recommendation (NBRR) a central problem in production NBR systems, where the goal is to predict which previously purchased items a user needs next, and when.

Most neural NBRR methods model user history as an ordered sequence of basket \emph{events}, where time is represented implicitly by basket index rather than calendar days~\cite{yu2020predicting,ranjan2025scalable}. Predictions are updated only when a new transaction is observed, and models cannot reason about elapsed time between visits. This basket-index formulation is a poor fit for production: scores do not evolve between transactions, and the model has no mechanism to distinguish a user who is 3 days into a 7-day repurchase cycle from one who is 6 days in.

Recent work showed that modeling purchase history as a calendar-time signal and learning per-item repurchase cycles via convolution substantially improves NBRR~\cite{katz2024personalized}. However, that approach generates personalized filters for each user--item pair via a hypernetwork and refines scores through pairwise item attention --- both quadratic in the user’s history size and infeasible at production scale (tens of millions of users, hundreds of thousands of items).

We propose \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding), which preserves the calendar-time cadence insight while remaining production-scalable. CASE uses \emph{shared} multi-scale convolutional filters --- spanning weekly, biweekly, monthly, seasonal, and trend horizons --- to encode recurring repurchase patterns without per-user parameterization. Cross-item dependencies within a user’s purchase history are captured through induced set attention blocks (ISAB)~\cite{lee2019set}, which reduce O($N^2$) pairwise attention to O($N{\cdot}K$) via a small set of learned induced points. Our contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=2pt]
  \item We identify the basket-index formulation as a fundamental limitation of existing NBRR for production deployment, and motivate calendar-time cadence as the appropriate inductive bias.
  \item We propose CASE, combining shared multi-scale CNN cadence encoding with induced set attention, requiring no per-user parameters and supporting efficient batch inference.
  \item We demonstrate up to 14\% relative Recall lift over a deployed production system on 10M+ users and 500K items, and consistent gains across three public benchmarks.
\end{itemize}

\section{CASE: Model Architecture}
\label{sec:model}

\textbf{Problem Formulation.}
Given user $u$ with purchase history $\mathcal{I}_u = \{i_1, \ldots, i_N\}$ (the set of $N$ distinct items purchased in the past $T$ calendar days), the NBRR task is to rank items in $\mathcal{I}_u$ by their likelihood of appearing in the next basket. We restrict recommendations to $\mathcal{I}_u$ since repurchase items dominate next-basket composition in grocery and retail settings.

\textbf{Calendar-Time History Representation.}
For each item $i \in \mathcal{I}_u$, we construct a binary purchase indicator $\mathbf{h}_i \in \{0,1\}^T$, where $h_{i,t} = 1$ if item $i$ was purchased on calendar day $t$ within a rolling $T{=}364$-day window. Unlike basket-index representations, this encoding directly exposes the elapsed time between purchases, enabling the model to detect item-level periodicity (e.g., a weekly-purchased item vs.\ a monthly one). Each item is additionally assigned a learned embedding $\mathbf{e}_i \in \mathbb{R}^{d_e}$ from a shared embedding table.

\textbf{Multi-Scale Temporal CNN.}
We apply a shared multi-scale 1D CNN to $\mathbf{h}_i$ to capture repurchase rhythms at five temporal resolutions: weekly ($k{=}7$), biweekly ($k{=}14$), monthly ($k{=}28$), seasonal ($k{=}91$), and trend ($k{=}182$) days. For each scale $k$, a single-channel Conv1d with stride $k$ produces $\lfloor T/k \rfloor$ non-overlapping window activations; all scales are concatenated and projected through two linear layers to a $d_c$-dimensional temporal feature $\mathbf{c}_i = \text{CNN}(\mathbf{h}_i) \in \mathbb{R}^{d_c}$. Crucially, all CNN filters are \emph{shared} across users, contrasting with CNN-HN~\cite{katz2024personalized} which generates personalized filters per user--item pair via a hypernetwork. The combined representation is $\mathbf{x}_i = [\mathbf{e}_i \,\|\, \mathbf{c}_i] \in \mathbb{R}^{d_e + d_c}$.

\textbf{Induced Set Attention Encoding.}
Items within $\mathcal{I}_u$ are an unordered set, and co-purchase patterns across items provide additional repurchase signals. We model these dependencies with an encoder based on Induced Set Attention Blocks (ISAB)~\cite{lee2019set}, which maps $\{\mathbf{x}_i\}_{i \in \mathcal{I}_u}$ to enriched representations $\{\mathbf{z}_i\}$ while remaining permutation-equivariant. ISAB introduces $K$ learnable \emph{induced points} as an intermediate attention target, reducing full pairwise attention from $O(N^2)$ to $O(N{\cdot}K)$. We stack two ISAB layers ($K{=}32$ induced points, $H{=}4$ attention heads, with layer normalization), yielding $\mathbf{z}_i \in \mathbb{R}^{d_h}$ for each item.

\textbf{Scoring and Training.}
Each $\mathbf{z}_i$ is passed through a two-layer MLP scorer to produce a scalar repurchase score $s_i$, and items are ranked by $s_i$ at inference. The model is trained with binary cross-entropy: for each observed basket transition, items in the next basket that also appear in $\mathcal{I}_u$ are positives; the remaining items in $\mathcal{I}_u$ are negatives. CASE has no per-user parameters --- the CNN filters, item embeddings, ISAB induced points, and scorer are all shared across users --- enabling batch inference that scales to millions of users with $O(1)$ additional memory per user.

\section{Experimental Setting}

\subsection{Datasets}

We evaluate on four datasets spanning grocery and retail domains (Table~\ref{tab:dataset_stats}). \textbf{Instacart}~\cite{yasserh_instacart_2017} is a large public grocery dataset with 18K users and 37K items across diverse categories. \textbf{TaFeng}~\cite{yu2020predicting} is a public Taiwanese supermarket dataset with moderate size and shorter user histories. \textbf{DC} (Dunnhumby\footnote{\url{https://www.dunnhumby.com/source-files/}}) is a loyalty-card retail dataset with a large user base but a small item catalog of 852 products. \textbf{Proprietary} is an internal large-scale grocery dataset with 10K users and 88K items. For all datasets, we follow the standard leave-one-out protocol: the last basket per user is held out as the test target; all preceding baskets are used for training. Calendar timestamps are preserved as absolute dates for the 364-day binary history encoding.

\begin{table}[h]
\centering
\small
\caption{Dataset statistics.}
\begin{tabular}{lrrrrr}
\toprule
Dataset & \#Users & \#Items & Avg.Bskt & Avg.Size & \#Baskets \\
\midrule
Instacart & 18,739 & 37,522 & 10.07 & 16.70 & 312,986 \\
TaFeng & 7,227 & 18,703 & 6.58 & 7.52 & 54,342 \\
DC & 123,935 & 852 & 1.60 & 14.16 & 1,754,890 \\
Proprietary & 10,308 & 88,812 & 11.72 & 43.00 & 443,232 \\
\bottomrule
\end{tabular}
\label{tab:dataset_stats}
\end{table}

\subsection{Baselines}

We compare against: \textbf{PersonalTop}, a non-parametric baseline that ranks a user’s historical items by global purchase frequency --- surprisingly strong due to the frequency bias in repurchase; \textbf{DNNTSP}~\cite{yu2020predicting}, a graph neural network model that learns item co-occurrence patterns for set-based next-basket prediction using basket-index timestamps; and \textbf{PIETSP}~\cite{ranjan2025scalable}, a scalable permutation-equivariant temporal set prediction model that extends DNNTSP with improved sequence modeling, also basket-index based.

\subsection{Evaluation and Implementation}

We report Precision@$K$, Recall@$K$, and NDCG@$K$ for $K \in \{1, 3, 5, 10\}$. CASE is implemented in PyTorch with item embedding dimension $d_e{=}256$, CNN output dimension $d_c{=}128$, and ISAB hidden dimension $d_h{=}256$. We train for 50 epochs with Adam (lr $= 10^{-3}$, weight decay $= 10^{-5}$) and dropout 0.1, using a 364-day purchase history window.

\section{Experimental Results}

% Style like your screenshot: dataset blocks + grouped K headers.
% Requires: \usepackage{booktabs,multirow}
% Optional: \usepackage{array} for better column spacing
\begin{table*}[t]
\centering
\small
\caption{Comparisons with different methods on Top-$K$ performance (repurchase next-basket recommendation). Higher is better. Best results within each dataset and metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{@{}ll|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}3$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c}{$K{=}10$} \\
& & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

% ========================= Instacart =========================
\multirow{4}{*}{Instacart}
& PersonalTop
& \underline{0.5019} & \underline{0.1173} & \underline{0.5019}
& \underline{0.4139} & \underline{0.2593} & \underline{0.4670}
& \underline{0.3574} & 0.3458 & \underline{0.4556}
& 0.2819 & 0.4821 & \textbf{0.4671} \\
& DNNTSP
& 0.4631 & 0.1025 & 0.4631
& 0.4002 & 0.2511 & 0.4447
& 0.3527 & \underline{0.3480} & 0.4427
& \underline{0.2842} & \underline{0.4943} & 0.4616 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.5339} & \textbf{0.1282} & \textbf{0.5339}
& \textbf{0.4508} & \textbf{0.2896} & \textbf{0.5072}
& \textbf{0.3971} & \textbf{0.3937} & \textbf{0.5040}
& \textbf{0.3118} & \textbf{0.5397} & \underline{0.5170} \\
\midrule

% ========================= Ta-Feng =========================
\multirow{4}{*}{TaFeng}
& PersonalTop
& 0.1771 & 0.0988 & 0.1771
& 0.1363 & 0.2100 & 0.2042
& 0.1104 & 0.2776 & 0.2263
& 0.0795 & 0.3839 & 0.2612 \\
& DNNTSP
& \underline{0.2387} & \underline{0.1632} & \underline{0.2387}
& 0.1675 & 0.3196 & 0.2869
& 0.1406 & 0.4255 & 0.3275
& 0.1086 & 0.5809 & 0.3739 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.2606} & \textbf{0.1708} & \textbf{0.2606}
& \textbf{0.1832} & \textbf{0.3306} & \textbf{0.3071}
& \textbf{0.1519} & \textbf{0.4318} & \textbf{0.3441}
& \textbf{0.1131} & \textbf{0.5932} & \textbf{0.3902} \\
\midrule

% ========================= DC =========================
\multirow{4}{*}{DC}
& PersonalTop
& \underline{0.3434} & \underline{0.2879} & \underline{0.3434}
& 0.2233 & 0.5424 & 0.4489
& 0.1672 & 0.6559 & 0.4828
& 0.1059 & 0.7838 & \underline{0.5000} \\
& DNNTSP
& 0.3264 & 0.2685 & 0.3264
& 0.2269 & 0.5485 & 0.4441
& 0.1710 & 0.6696 & 0.4790
& 0.1085 & 0.8057 & 0.4931 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.3906} & \textbf{0.3300} & \textbf{0.3906}
& \textbf{0.2512} & \textbf{0.6120} & \textbf{0.5094}
& \textbf{0.1845} & \textbf{0.7236} & \textbf{0.5407}
& \textbf{0.1139} & \textbf{0.8446} & \textbf{0.5479} \\
\midrule

% ========================= Proprietary =========================
\multirow{4}{*}{Proprietary}
& PersonalTop
& \underline{0.3686} & \underline{0.0940} & \underline{0.3686}
& 0.2744 & 0.1691 & 0.3248
& 0.2314 & 0.2206 & 0.3151
& 0.1772 & 0.2981 & 0.3126 \\
& DNNTSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.3849} & \textbf{0.1004} & \textbf{0.3849}
& \textbf{--} & \textbf{--} & \textbf{--} % TODO: K=3 not in ablation, needs re-run
& \textbf{0.2593} & \textbf{0.2535} & \textbf{0.3495}
& \textbf{0.2009} & \textbf{0.3438} & \textbf{0.3500} \\
\bottomrule
\end{tabular}
\label{tab:topk_grouped_results}
\end{table*}

\subsection{Main Results}

Table~\ref{tab:topk_grouped_results} shows results across all datasets. CASE outperforms all baselines on every public dataset at every $K$. The gains are most pronounced at $K{=}1$: on Instacart CASE improves Precision@1 from 0.502 (PersonalTop) to 0.534 (+6.4\%), and on DC from 0.343 to 0.391 (+14.0\%). PersonalTop ranks second in most settings, reflecting the strong frequency signal in repurchase behavior; however, it cannot distinguish items that are due for repurchase from those that are not, whereas CASE's calendar-time encoding captures precisely this timing distinction. DNNTSP, despite using learned item co-occurrence, underperforms PersonalTop at low $K$ on Instacart and DC, where cadence timing is decisive. On the Proprietary dataset, CASE consistently outperforms PersonalTop across all $K$ values, and delivers a 14\% relative Precision and Recall lift over the deployed production system at $K{=}1$ (Table~\ref{tab:lift}).

\subsection{Ablation Study}

\begin{table*}[t]
\centering
\small
\caption{Ablation study on the Proprietary dataset. Higher is better. Best results per metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Model Components}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c|}{$K{=}10$}
& \multicolumn{3}{c}{$K{=}20$} \\
& Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

CNN w/ Set Attention
& \underline{0.3849} & \underline{0.1004} & \underline{0.3849}
& \underline{0.2593} & \textbf{0.2535} & \textbf{0.3495}
& \textbf{0.2009} & \textbf{0.3438} & \textbf{0.3500}
& \textbf{0.1488} & \textbf{0.4645} & \textbf{0.3714} \\

  + Set pooling
& 0.3724 & 0.0991 & 0.3724
& 0.2563 & 0.2492 & 0.3450
& \underline{0.1980} & \underline{0.3423} & \underline{0.3460}
& \underline{0.1445} & \underline{0.4539} & \underline{0.3648} \\


CNN w/ Set Perm Mean
& \textbf{0.3897} & \textbf{0.1038} & \textbf{0.3897}
& \textbf{0.2597} & \underline{0.2505} & \underline{0.3489}
& 0.1974 & 0.3353 & 0.3459
& 0.1433 & 0.4426 & 0.3626 \\

% CNN w/ Set attention Encoder plus pooling
% & 0.3724 & 0.0991 & 0.3724
% & 0.2563 & 0.2492 & 0.3450
% & \underline{0.1980} & \underline{0.3423} & \underline{0.3460}
% & \underline{0.1445} & \underline{0.4539} & \underline{0.3648} \\

CNN w/o any set encoder
& 0.3767 & 0.0984 & 0.3767
& 0.2525 & 0.2455 & 0.3399
& 0.1974 & 0.3406 & 0.3434
& 0.1435 & 0.4440 & 0.3598 \\

% No\_SetTran\_PMA
% & 0.3772 & 0.0993 & 0.3772
% & 0.2571 & 0.2493 & 0.3450
% & 0.1977 & 0.3408 & 0.3449
% & 0.1463 & \underline{0.4567} & \underline{0.3661} \\

Set Attention w/o CNN
& 0.2226 & 0.0628 & 0.2226
& 0.1292 & 0.1454 & 0.1872
& 0.1020 & 0.2092 & 0.1921
& 0.0754 & 0.2912 & 0.2064 \\

\bottomrule
\end{tabular}
\end{table*}


\begin{table}[t]
\centering
\small
\caption{Relative lift (\%) of the proposed model over the production model.}
\begin{tabular}{c|ccc}
\toprule
K & Precision & Recall & NDCG \\
\midrule
1  & +14.11\% & +14.62\% & +14.11\% \\
5  & +8.63\%  & +9.90\%  & +10.46\% \\
10 & +6.78\%  & +7.95\%  & +9.40\% \\
20 & +5.27\%  & +6.32\%  & +8.75\% \\
\bottomrule
\end{tabular}
\label{tab:lift}
\end{table}

Table~\ref{tab:lift} confirms sustained gains across all $K$ values, with the relative improvement tapering from 14\% at $K{=}1$ to 5\% at $K{=}20$ as the task becomes easier. The ablation study isolates contributions of each component. \textbf{Removing the temporal CNN} (``Set Attention w/o CNN'') causes the largest degradation: Precision@1 drops from 0.385 to 0.223 ($-$42\%), establishing calendar-time cadence encoding as the dominant signal. \textbf{Removing ISAB} (``CNN w/o any set encoder'') consistently reduces performance, particularly at larger $K$, confirming that cross-item dependencies contribute meaningful signal. \textbf{Adding dual scoring} (``+ Set pooling'') slightly hurts performance, indicating that the per-item intrinsic score from ISAB is self-sufficient and the global compatibility term introduces noise rather than signal. Both ISAB and PermEqMean perform comparably, though ISAB maintains a consistent advantage at $K{\geq}5$.

\section{Conclusion}

We presented CASE, a cadence-aware set encoding model for large-scale next basket repurchase recommendation. By representing each item's purchase history as a binary calendar-time indicator and applying shared multi-scale convolutional filters, CASE captures item-level repurchase rhythms without per-user parameters. Induced set attention then enriches item representations with cross-item co-purchase context at sub-quadratic cost. CASE achieves consistent improvements over strong baselines across three public benchmarks and delivers up to 14\% relative Recall and Precision lift over a deployed production system serving millions of users. These results demonstrate that decoupling cadence encoding from user-specific parameterization is both practically necessary and empirically effective for production-scale NBRR.

% \begin{table}[!htbp]
% \centering
% \caption{Accuracy and error metrics across context levels, with best ML and statistical baselines. Bold values indicate the overall best performance across all models. Underlined values indicate the best among LLMs.}
% \label{tab:llm_combined_metrics_onecol}
% \setlength{\tabcolsep}{2pt}
% \renewcommand{\arraystretch}{0.92}
% \begin{tabular}{lcccccc}
% \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} &
% \multicolumn{3}{c}{\textbf{Accuracy Metrics ↑}} &
% \multicolumn{3}{c}{\textbf{Error Metrics ↓}} \\
% \cmidrule(lr){2-4}\cmidrule(lr){5-7}
% & \textbf{TA@0} & \textbf{TA@1} & \textbf{TA@2} &
% \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} \\
% \midrule
% \multicolumn{7}{c}{\textbf{Proprietary data}} \\
% \midrule
% GPT-4o-Z    & 5.75 & 12.72 & 18.65 & 23.66 & 15.50 & 73.03 \\
% GPT-4o-M   & 6.13 & 13.83 & 19.92 & 22.95 & 14.76 & 66.78 \\
% GPT-4o-H   & 5.32 & 12.72 & 18.48 & 24.83 & 16.39 & 76.59 \\
% \midrule
% Gemini-2.5-Z  & 6.38 & 13.57 & 19.68 & 23.44 & 15.17 & 63.79 \\
% Gemini-2.5-M  & 6.15 & 13.95 & 19.72 & 23.91 & 15.39 & 67.79 \\
% Gemini-2.5-H  & 6.20 & 13.42 & 19.25 & 24.20 & 15.66 & 71.38 \\
% \midrule
% Claude-3.5-Z & 5.98 & 13.50 & 19.72 & 22.26 & 14.17 & 64.27 \\
% Claude-3.5-M & 6.55 & 14.58 & \underline{20.97} & \underline{21.93} & \underline{13.85} & \underline{57.45} \\
% Claude-3.5-H & \underline{6.75} & \underline{14.63} & 20.95 & 22.11 & 14.11 & 59.61 \\
% \midrule
% ML Best      & \textbf{9.48} & \textbf{22.98} & \textbf{33.93} & \textbf{9.97} & \textbf{7.18} & \textbf{29.92} \\
% Stat Best    & 4.42 & 13.15 & 20.32 & 22.46 & 14.25 & 55.41 \\
% \midrule
% \multicolumn{7}{c}{\textbf{Instacart data}} \\
% \midrule
% GPT-4o-Z   & 6.54 & 15.46 & 22.16 & 30.11 & 16.13 & 77.39 \\
% GPT-4o-M   & \underline{7.32} & 16.04 & 22.98 & 28.56 & 15.09 & 66.80 \\
% GPT-4o-H   & 6.00 & 14.12 & 20.12 & 31.05 & 17.01 & 84.03 \\
% \midrule
% Gemini-2.5-Z & 7.30 & 15.76 & 22.82 & 28.85 & 15.27 & 64.13 \\
% Gemini-2.5-M & 7.28 & \underline{16.64} & \underline{23.06} & 28.36 & 15.05 & \underline{59.43} \\
% Gemini-2.5-H & 6.26 & 14.80 & 21.36 & 29.46 & 16.17 & 74.38 \\
% \midrule
% Claude-3.5-Z & 6.22 & 14.48 & 22.20 & \underline{26.88} & 14.18 & 67.71 \\
% Claude-3.5-M & 6.02 & 14.24 & 21.82 & 26.92 & \underline{13.93} & 62.29 \\
% Claude-3.5-H & 6.92 & 15.10 & 22.44 & 27.50 & 14.42 & 64.31 \\
% \midrule
% ML Best      & \textbf{8.46} & \textbf{22.62} & \textbf{33.42} & \textbf{9.17} & \textbf{6.55} & \textbf{35.04} \\
% Stat Best    & 5.90 & 15.34 & 23.00 & 27.97 & 14.52 & 56.34 \\
% \bottomrule
% \end{tabular}
% \end{table}


% We assess model performance using three categories of metrics: (1) standard regression metrics including Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). (2) business-oriented accuracy metrics, defined as Tolerance Accuracy (TA@k) which is the proportion of predictions where the absolute error is $k$ days or less (i.e., $|y - \hat{y}| \le k$). This metric directly measures the model's utility in a real-world setting where exact precision is not always required. We report TA@0 (exact-day accuracy), TA@1 (±1-day accuracy), and TA@2 (±2-day accuracy). (3) Deployment performance is measured by the cost and latency required to generate a prediction.
%  % and the coefficient of determination ($R^2$)


% \textbf{RQ1: LLMs vs. Traditional ML}
% A primary finding from \ref{tab:llm_combined_metrics_onecol}, consistent across both the proprietary and Instacart datasets, is the ML model's dominant performance. It achieves over 50\% improvement on most key metrics compared to the best-performing LLM. For instance, on the proprietary dataset, the ML model's MAPE of 29.92\% is 92.0\% better than the best-performing LLM (Claude-3.5 Medium at 57.45\%). On the business-critical "TA@1" metric, the ML model scores 22.98\%, beating the best LLM's 14.63\% (Claude-3.5 High) by 57.1\%. This performance gap is likely due to a fundamental "impedance mismatch" for the LLM. The ML model directly operates on structured numerical features, whereas the LLM must first translate qualitative linguistic descriptions into an internal representation before performing regression, introducing significant error. Notably, the disparity in performance is significantly larger for error metrics than for accuracy metrics, as LLMs' performance on TA@k is competitive. This suggests that while LLMs fail at quantitative precision (i.e. pinpointing the exact day), they are better at approximate temporal classification. This aligns with the business goal for replenishment, where knowing a user will buy "in the next 48 hours" is often sufficient.

% Another notable observation is that LLMs outperform the “stat best” median baseline across both datasets and metrics, indicating that they extract additional signal from contextual descriptions beyond simple historical central tendency. This suggests that LLMs incorporate contextual cues rather than merely reproducing medians, yielding more informative estimates, especially for approximation rather than exact-day prediction. These results indicate that LLMs have useful but limited temporal reasoning capabilities.


% \textbf{RQ2: The Impact of Context on LLM Performance} Our second research question examines how different levels of context affect LLM performance. Across both datasets, medium-level context, including basic product attributes, simple statistics, and concise user history, consistently improves both model metrics and business-oriented accuracy. In contrast, high-level narrative context often degrades performance, sometimes matching the zero-context baseline. This suggests a context-as-noise effect: focused, decision-relevant information aids temporal reasoning, whereas semantically rich but temporally irrelevant details distract the model. As illustrated in Figure \ref{fig:casestudy}, green highlights show that medium-context prompts encourage the model to rely on stable quantitative cues, producing correct or near-correct predictions, while red highlights show that high-context prompts surface narrative details that models overweight, diverting attention from the underlying temporal structure and leading to erroneous predictions.

% Across LLM families, Claude-3.5 performs most consistently and achieves the best results on the internal dataset. On the Instacart dataset, Claude-3.5 and Gemini-2.5 achieve top performance on different metrics. In terms of efficiency, GPT-4o is the fastest and cheapest (around 1.2--1.4s latency, >\$0.003 per call), while Claude-3.5 is moderately slower (around 2--4s) and 3$\times$ more expensive, and Gemini-2.5 has the highest latency (around 15--19s) with lower overall accuracy.


% Our study presents a framework for time-interval prediction, establishes strong baselines, and positions LLMs between statistical and machine-learning models. We show that targeted and compact context can aid LLM reasoning, while excessive narrative detail can obscure the temporal signal. These findings highlight both the potential and limitations of LLMs for structured temporal inference and point toward hybrid models combining statistical precision with linguistic flexibility.




% \clearpage
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
