%%

%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

% \documentclass[sigconf]{acmart}
\documentclass[sigconf,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}

% \documentclass[sigconf]{acmart}

\copyrightyear{2026}
\acmYear{2026}
\setcopyright{cc}
\setcctype{by}
% \acmConference[SIGIR '26]{Proceedings of the ACM Web Conference 2026}{April 13--17, 2026}{Dubai, United Arab Emirates}
% \acmBooktitle{Proceedings of}
% \acmPrice{}
% \acmDOI{xxxxxxxxxxx}
% \acmISBN{xxxxxxxxxxxx}

% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.



\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}

% % COMMENTS
\newif\ifcomment
\commenttrue % comment out to hide comments
\providecommand{\YC}[1]{\ifcomment{\small \color{blue} [YC: #1]}\fi} %Yanan
\providecommand{\AR}[1]{\ifcomment{\small \color{red} [AR: #1]}\fi} %Ashish

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{CASE: Cadence-Aware Set Encoding for Large-Scale Next Basket Repurchase Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Anonymous Author(s)}
% \date{}
% \settopmatter{authorsperrow=4}

% \author{Anonymous Author(s)}


\author{Yanan Cao}
\authornote{Highlighted authors contributed equally to this research.}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{yanan.cao@walmart.com}

\author{Ashish Ranjan}
\authornotemark[1]
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ashish.ranjan0@walmart.com}

\author{Sinduja Subramaniam}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{sinduja.subramaniam@walmart.com}

\author{Evren Korpeoglu}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ekorpeoglu@walmart.com}

\author{Kaushiki Nag}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kaushiki.nag@walmart.com}

\author{Kannan Achan}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kannan.achan@walmart.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Yanan Cao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Repurchase behavior is a primary signal in large-scale retail recommendation, particularly in categories with frequent replenishment: many items in a user’s next basket were previously purchased and their timing follows stable, item-specific cadences. Yet most next basket recommendation models represent history as a sequence of discrete basket events indexed by visit order, which cannot explicitly model elapsed calendar time or update predicted item rankings as calendar days pass between purchases. We present \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding for next basket repurchase recommendation), which decouples item-level cadence learning from cross-item interaction, enabling explicit calendar-time modeling while remaining production-scalable. CASE represents each item’s purchase history as a calendar-time signal over a fixed horizon, applies shared multi-scale temporal convolutions to capture recurring rhythms, and uses induced set attention to model cross-item dependencies with sub-quadratic complexity, allowing efficient batch inference at scale. Across three public benchmarks and a proprietary dataset, CASE consistently improves Precision, Recall, and NDCG at multiple cutoffs compared to strong next basket prediction baselines. In a production-scale evaluation with tens of millions of users and a large item catalog, CASE achieves up to 8.6\% relative Precision and 9.9\% Recall lift at $K{=}5$, demonstrating that scalable cadence-aware modeling yields measurable gains in both benchmark and industrial settings.
\end{abstract}



%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Sequential Modeling}
\ccsdesc[500]{Computing methodologies~Temporal Modeling}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Next Basket Recommendation, Sequential Modeling, Temporal Prediction}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\linewidth]{figure1.jpg}
    \caption{
    CASE architecture. Item purchase histories are encoded as binary $T$-day calendar-time signals. Shared multi-scale Conv1d kernels from weekly through seasonal extract cadence embeddings $\mathbf{c}_i$, concatenated with item embeddings $\mathbf{e}_i$ and processed by Induced Set Attention Blocks (ISAB) to capture cross-item dependencies at sub-quadratic cost. A final MLP outputs per-item repurchase scores.
    }    
    \label{fig:diagram}
\end{figure*}

In large-scale retail platforms with frequent replenishment behavior, a substantial fraction of items in a user’s next basket were previously purchased. Their repurchase timing often follows stable, item-specific cadences, such as milk purchased weekly and cleaning products purchased monthly. Thus, accurate timing is critical for user experience: recommending too early makes suggestions appear irrelevant, whereas recommending too late risks missing the purchase opportunity. This makes Next Basket Repurchase Recommendation (NBRR) a central task in production retail recommendation systems, where the goal is to predict which previously purchased items a user will need next.

Most neural NBRR methods model user history as an ordered sequence of basket events, where time is represented implicitly by basket index rather than by elapsed calendar time~\cite{yu2020predicting, yu2023predicting}. As a result, baskets on days 1, 8, and 36 are represented identically from days 1, 2, and 3 under the same three-step representation, and predictions are updated only when a new transaction occurs, leaving scores static between purchases and unable to reflect whether an item is overdue or not yet due. This creates a fundamental mismatch with production deployment: a model that does not explicitly account for elapsed time cannot get meaningfully refreshed between user sessions and provides adaptive signal for users whose purchase frequency varies over time. A related class of KNN-based methods models basket index with recency decay, retrieving similar users and aggregating their purchase patterns, and has shown strong performance on next basket prediction benchmarks~\cite{hu2020modeling}. However, these approaches require computing user-to-user similarities at inference time, incurring $O(|\mathcal{U}| \cdot d)$ cost per query (where $|\mathcal{U}|$ is the number of users and $d$ is the embedding dimension), followed by aggregation over neighbors’ items. This becomes infeasible at the scale of tens of millions of users.

Some recent works have explored calendar-time representations for NBRR. One line represents item history as a binary time series and learns repurchase cycles via convolution~\cite{katz2024personalized}; however, it relies on user-specific convolution filters parameterization and quadratic item-interaction modules, limiting scalability at production scale. A complementary approach~\cite{ranjan2025scalable} achieves scalable set-level modeling via permutation-equivariant aggregation over calendar-time membership, but does not explicitly extract multi-scale cadence patterns at the item level. 

In this paper, we propose \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding for Large-scale Next Basket Repurchase Recommendation), which explicitly models repurchase cadence in calendar time while remain scalable at production scale. CASE applies shared multi-scale convolutional filters to capture item-level recurring patterns across weekly, biweekly, monthly, seasonal, and trend horizons. Cross-item dependencies within a user’s purchase history are encoded through induced set attention blocks (ISAB)~\cite{lee2019set}, reducing the quadratic complexity of full self-attention. Our contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=2pt]
  \item We identify the basket-index formulation as a structural limitation of existing NBRR for production deployment, and motivate calendar-time cadence as a more suitable formulation.
  \item We propose CASE, combining shared multi-scale CNN-based cadence learning with induced set attention, requiring no per-user parameters and enabling efficient batch inference.
  \item We demonstrate up to 8.6\% relative Precision and 9.9\% Recall lift over a deployed production system on tens of millions of users and a large item catalog, along with consistent gains across three public benchmarks.
  \item Ablation shows cadence modeling dominates performance, with only modest degradation without item embeddings, reducing dependence on large, frequently refreshed embedding tables and enhancing production scalability.
\end{itemize}

  % \item We demonstrate consistent gains across three public benchmarks, and through ablation show that cadence modeling dominates performance, with only modest degradation without item embeddings, reducing dependence on large, frequently refreshed embedding tables and improving scalability.
  % \item In a deployed production system serving tens of millions of users and a large item catalog, CASE achieves up to 8.6\% relative Precision and 9.9\% Recall lift.


\section{CASE: Model Architecture}
\label{sec:model}

% \textbf{Problem Formulation.}
Let $\mathcal{B}_u = (B_1, B_2, \ldots, B_L)$ denote the ordered basket sequence for user $u$, where $B_l \subseteq \mathcal{I}$ is purchased on calendar date $d_l$. The NBRR task ranks items in the repurchase history $\mathcal{I}_u = \bigcup_l B_l$ by their likelihood of appearing in $B_{L+1}$. Figure~\ref{fig:diagram} provides an overview of the CASE architecture.

\textbf{Calendar-Time History Representation.}
For each item $i \in \mathcal{I}_u$, we construct a binary purchase indicator $\mathbf{h}_i \in \{0,1\}^T$ over a rolling $T$-day calendar window, where $h_{i,t}{=}1$ if item $i$ was purchased on day $t$. Unlike basket-index encoding, this preserves actual inter-purchase intervals, enabling the model to distinguish items with different repurchase cadences and capture seasonal effects.

\textbf{Multi-Scale Temporal CNN.}
Shared multi-scale Conv1d filters are applied over $\mathbf{h}_i$ at five predefined kernel sizes: weekly ($k{=}7$), biweekly ($k{=}14$), monthly ($k{=}28$), seasonal ($k{=}91$), and trend ($k{=}182$), each with stride $k$ (non-overlapping windows). The $\lfloor T/k \rfloor$ activations per scale are concatenated across all scales and projected through two FC layers with ReLU to yield $\mathbf{c}_i \in \mathbb{R}^{d_c}$. This multi-resolution design captures temporal patterns at multiple horizons, enabling CASE to model periodicity and trends using population-wide shared weights.

\textbf{Induced Set Attention Encoding.} 
The combined representation $\mathbf{x}_i = [\mathbf{c}_i \,\|\, \mathbf{e}_i]$, where $\mathbf{e}_i \in \mathbb{R}^{d_e}$ is a learned item embedding, is fed into a Set Transformer encoder~\cite{lee2019set} to model cross-item dependencies among a user’s unordered repurchase candidates. We use Induced Set Attention Blocks (ISAB), which reduce $O(n^2)$ pairwise attention to $O(nK)$, where $n = |\mathcal{I}_u|$ is the number of candidate items for user $u$, via $K$ learnable induced points that first attend to the item set, then items attend back. Two ISAB layers with $K{=}32$ and $H{=}4$ heads produce enriched representations $\mathbf{z}_i \in \mathbb{R}^{d_h}$.

% \YC{don't be repeatetive on O complexity analysis}

% The combined representation $\mathbf{x}_i = [\mathbf{c}_i \,\|\, \mathbf{e}_i]$, where $\mathbf{e}_i \in \mathbb{R}^{d_e}$ is a learned item embedding capturing semantic identity, serves as input to the set encoder. Since a user's repurchase candidates form an unordered set, we capture cross-item co-purchase dependencies using a Set Transformer encoder~\cite{lee2019set} with Induced Set Attention Blocks (ISAB). ISAB replaces quadratic self-attention with interaction through $K$ learnable induced points $\mathbf{I} \in \mathbb{R}^{K \times d_h}$, yielding $O(nK)$ complexity for a user with $n = |\mathcal I_u|$ candidate items. Two ISAB layers with $K{=}32$ and $H{=}4$ heads produce enriched representations $\mathbf{z}_i \in \mathbb{R}^{d_h}$ from $\mathbf{x}_i$.

\textbf{Scoring and Training.}
Each $\mathbf{z}_i$ is passed through a two-layer MLP to produce a scalar score $s_i$; items are ranked by $s_i$ at inference. Training minimizes binary cross-entropy, with items in $B_{l+1} \cap \mathcal{I}_u$ as positives and all others as negatives.
% All parameters are shared across users, making CASE's memory footprint independent of user population size.

% \textbf{Computational Complexity.}
% For a user with $n = |\mathcal I_u|$ candidate items, multi-scale temporal convolutions over a $T$-day horizon require $O(nT)$ computation (treating the number of scales and channels as constants), while ISAB set encoding requires $O(nK)$. The overall per-user complexity is therefore $O(n(T+K))$, enabling efficient batch inference over large user populations.


\begin{table*}[t]
\centering
\small
\caption{Comparisons on Top-$K$ performance for next basket repurchase recommendation. Higher is better. Best results per dataset and metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{@{}ll|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods}
& \multicolumn{3}{c|}{$k{=}1$}
& \multicolumn{3}{c|}{$k{=}3$}
& \multicolumn{3}{c|}{$k{=}5$}
& \multicolumn{3}{c}{$k{=}10$} \\
& & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

% ========================= TaFeng =========================
\multirow{5}{*}{TaFeng}
% & Personal\_Top
% & 0.1771 & 0.0988 & 0.1771
% & 0.1363 & 0.2100 & 0.2042
% & 0.1104 & 0.2776 & 0.2263
% & 0.0795 & 0.3839 & 0.2612 \\
& TIFUKNN
& 0.2146 & 0.1201 & 0.2146
& 0.1639 & 0.2646 & 0.2503
& 0.1332 & 0.3443 & 0.2769
& 0.0995 & 0.5007 & 0.3228 \\
& DNNTSP
& 0.2387 & 0.1632 & 0.2387
& \underline{0.1675} & \underline{0.3196} & 0.2869
& 0.1406 & \underline{0.4255} & 0.3275
& 0.1086 & 0.5809 & 0.3739 \\
& BERT4NBR
& 0.2316 & 0.1527 & 0.2316
& 0.1662 & 0.3035 & 0.2783
& 0.1399 & 0.3986 & 0.3120
& 0.1079 & 0.5753 & 0.3638 \\
& PIETSP
& \underline{0.2507} & \underline{0.1752} & \underline{0.2507}
& 0.1670 & 0.3165 & \underline{0.2903}
& \underline{0.1421} & 0.4223 & \underline{0.3317}
& \underline{0.1094} & \underline{0.5853} & \underline{0.3801} \\
& CASE
& \textbf{0.2897} & \textbf{0.1877} & \textbf{0.2897}
& \textbf{0.1944} & \textbf{0.3471} & \textbf{0.3260}
& \textbf{0.1539} & \textbf{0.4361} & \textbf{0.3559}
& \textbf{0.1157} & \textbf{0.5953} & \textbf{0.4021} \\
\midrule

% ========================= DC =========================
\multirow{5}{*}{DC}
% & Personal\_Top
% & 0.3434 & 0.2879 & 0.3434
% & 0.2233 & 0.5424 & 0.4489
% & 0.1672 & 0.6559 & 0.4828
% & 0.1059 & 0.7838 & 0.5000 \\
& TIFUKNN
& 0.3843 & 0.3245 & 0.3843
& 0.2498 & 0.6081 & \underline{0.5051}
& \underline{0.1843} & 0.7229 & 0.5387
& \underline{0.1140} & 0.8442 & \underline{0.5473} \\
& DNNTSP
& 0.3264 & 0.2685 & 0.3264
& 0.2269 & 0.5485 & 0.4441
& 0.1710 & 0.6696 & 0.4790
& 0.1085 & 0.8057 & 0.4931 \\
& BERT4NBR
& 0.3674 & 0.3102 & 0.3674
& 0.2392 & 0.5828 & 0.4820
& 0.1787 & 0.7009 & 0.5141
& 0.1121 & 0.8292 & 0.5183 \\
& PIETSP
& \underline{0.3886} & \underline{0.3291} & \underline{0.3886}
& \underline{0.2499} & \underline{0.6094} & 0.5023
& 0.1841 & \underline{0.7229} & \underline{0.5389}
& 0.1092 & \underline{0.8443} & 0.5467 \\
& CASE
& \textbf{0.3904} & \textbf{0.3296} & \textbf{0.3904}
& \textbf{0.2515} & \textbf{0.6113} & \textbf{0.5089}
& \textbf{0.1846} & \textbf{0.7230} & \textbf{0.5401}
& \textbf{0.1143} & \textbf{0.8468} & \textbf{0.5487} \\
\midrule

% ========================= Instacart =========================
\multirow{5}{*}{Instacart}
% & Personal\_Top
% & 0.5019 & 0.1173 & 0.5019
% & 0.4139 & 0.2593 & 0.4670
% & 0.3574 & 0.3458 & 0.4556
% & 0.2819 & 0.4821 & 0.4671 \\
& TIFUKNN
& \textbf{0.5489} & \textbf{0.1340} & \textbf{0.5489}
& \underline{0.4541} & \underline{0.2880} & \underline{0.5131}
& \underline{0.3930} & \underline{0.3878} & \underline{0.5037}
& \underline{0.3115} & \underline{0.5371} & \underline{0.5189} \\
& DNNTSP
& 0.4631 & 0.1025 & 0.4631
& 0.4002 & 0.2511 & 0.4447
& 0.3527 & 0.3480 & 0.4427
& 0.2842 & 0.4943 & 0.4616 \\
& BERT4NBR
& 0.3576 & 0.0812 & 0.3576
& 0.2875 & 0.1910 & 0.3276
& 0.2492 & 0.2608 & 0.3231
& 0.2047 & 0.3811 & 0.3397 \\
& PIETSP
& 0.5210 & 0.1222 & 0.5210
& 0.4424 & 0.2772 & 0.4951
& 0.3838 & 0.3773 & 0.4891
& 0.2986 & 0.5429 & 0.5155 \\
& CASE
& \underline{0.5478} & \underline{0.1325} & \underline{0.5478}
& \textbf{0.4551} & \textbf{0.2901} & \textbf{0.5135}
& \textbf{0.3989} & \textbf{0.3949} & \textbf{0.5086}
& \textbf{0.3155} & \textbf{0.5464} & \textbf{0.5241} \\
\midrule

% ========================= Proprietary =========================
\multirow{5}{*}{Proprietary}
% & Personal\_Top
% & 0.3672 & 0.0921 & 0.3672
% & 0.2744 & 0.1694 & 0.3251
% & 0.2310 & 0.2205 & 0.3152
% & 0.1766 & 0.2962 & 0.3121 \\
& TIFUKNN
& \underline{0.3856} & \underline{0.0971} & \underline{0.3856}
& \underline{0.3010} & \underline{0.1913} & \underline{0.3534}
& \underline{0.2615} & \underline{0.2550} & \underline{0.3503}
& \textbf{0.2038} & \textbf{0.3547} & \textbf{0.3543} \\
& DNNTSP
& 0.2661 & 0.0614 & 0.2661
& 0.2154 & 0.1269 & 0.2479
& 0.1852 & 0.1703 & 0.2420
& 0.1474 & 0.2510 & 0.2456 \\
& BERT4NBR
& 0.2288 & 0.0674 & 0.2288
& 0.1522 & 0.1132 & 0.1925
& 0.1275 & 0.1453 & 0.1878
& 0.0979 & 0.2028 & 0.1908 \\
& PIETSP
& 0.3803 & 0.0959 & 0.3803
& 0.2913 & 0.1814 & 0.3431
& 0.2422 & 0.2311 & 0.3301
& 0.1822 & 0.3094 & 0.3245 \\
& CASE
& \textbf{0.3871} & \textbf{0.1007} & \textbf{0.3871}
& \textbf{0.3046} & \textbf{0.1921} & \textbf{0.3571}
& \textbf{0.2661} & \textbf{0.2550} & \textbf{0.3509}
& \underline{0.2017} & \underline{0.3448} & \underline{0.3514} \\
\bottomrule
\end{tabular}
\label{tab:main_results}
\end{table*}

\section{Experimental Setting}

\subsection{Datasets}

We evaluate on four datasets spanning grocery and retail domains, chosen to cover a range of catalog sizes, basket densities, and user scales.

% \begin{table}[t]
% \centering
% \small
% \caption{Dataset statistics.}
% \begin{tabular}{lrrrrr}
% \toprule
% Dataset & \#U & \#I & Avg\_Size\_basket_szie & Avg\_Basket/User & \#B \\
% \midrule
% DC & 123,935 & 852 & 1.60 & 14.16 & 1,754,890 \\
% Instacart & 18,739 & 37,522 & 10.07 & 16.70 & 312,986 \\
% Ta-Feng & 7,227 & 18,703 & 6.58 & 7.52 & 54,342 \\
% Retail-Prop & 10,308 & 88,812 & 11.72 & 43.00 & 443,232 \\
% \bottomrule
% \end{tabular}
% \label{tab:dataset_stats}
% \end{table}

\textbf{Instacart}~\cite{yasserh_instacart_2017} is a public online grocery dataset with 18,739 users, 37,522 products, and an average of 16.7 baskets per user with 10.01 items each. \textbf{TaFeng}~\cite{yu2020predicting} is a Taiwanese cash-and-carry supermarket dataset with 7,227 users and 18,703 items, whose shorter histories (7.52 baskets per user) and smaller baskets (6.58 items) make it a challenging setting for cadence models. \textbf{DC} is derived from the Dunnhumby ``Carbo-Loading'' database\footnote{\url{https://www.dunnhumby.com/source-files/}} and contains 123,935 users but only 852 distinct products, with very small baskets (1.60 products per basket). \textbf{Proprietary} data is randomly sampled from our internal large-scale grocery dataset, having 10,308 users across a large amount of 88,812 items, with rich histories of 11.72 items per basket and 43 baskets per user on average. 

For all datasets, we perform a user-level train–test split with leave-one-out labeling, using each user’s last order as the target basket. Candidates are restricted to items previously purchased by the user. we preserve calendar timestamps when available (TaFeng and Proprietary); otherwise (Instacart and DC), we reconstruct relative dates from inter-order intervals to build the $T$-day binary history representation.

% For all datasets, we conduct train-test split based on customers and construct labels by leave-one-out: each user’s items in last order is the target basket. Item candidate is defined as the intersection of target basket with the user's history baskets. We preserve calendar timestamps when available (e.g. TaFeng and Proprietary). For datasets without explicit order dates (e.g., Instacart and DC), we construct relative dates based on provided number of days between orders to build the $T$-day binary history representation. 

\subsection{Baselines}

% \YC{maybe can extend baseline model description a little since we have space}
We compare CASE against four baselines: \textbf{TIFUKNN}~\cite{hu2020modeling}, a KNN method that builds temporally decayed item vectors per user and aggregates scores from similar users, achieving strong repurchase performance but at high inference cost that is non-scalable in production; \textbf{DNNTSP}~\cite{yu2020predicting}, which applies GNN aggregation over an item co-occurrence graph with basket-index temporal attention; \textbf{BERT4NBR}~\cite{li2023masked}, an adaptation of BERT4Rec to NBR setting by applying bidirectional self-attention over basket-indexed purchase sequences; and \textbf{PIETSP}~\cite{ranjan2025scalable}, which applies permutation-equivariant mean pooling on top of time-step based item history and for scalable set-level aggregation, but without multi-scale temporal feature extraction at the item level.


\subsection{Evaluation and Implementation}

We report three metrics at $K \in \{1, 3, 5, 10\}$. \textbf{Precision@$K$} measures the fraction of recommended items that are relevant, which is the primary metric in user-facing recommendation systems; \textbf{Recall@$K$} measures the fraction of all relevant items captured in the top-$K$ list; \textbf{NDCG@$K$} is a ranking-aware metric that assigns higher scores to positive items appearing higher in the list. 

CASE is implemented in PyTorch with item embedding dimension $d_e{=}128$, CNN output dimension $d_c{=}128$, ISAB hidden dimension $d_h{=}256$, $K{=}32$ induced points, and $H{=}4$ attention heads. We train for 30 epochs using the Adam optimizer with learning rate $10^{-3}$, weight decay $10^{-5}$, and batch size 64. Dropout of 0.1 is applied after each ISAB layer and in the MLP scorer. Code is available at 
\url{https://github.com/ycao21/CASE_NBRR}.


\section{Experimental Results}

\begin{table*}[t]
\centering
\small
\caption{Ablation study on the Instacart dataset. Higher is better. Best results are in \textbf{bold}; second best are \underline{underlined}.}
\label{tab:ablation}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Model Components}
& \multicolumn{3}{c}{$k{=}1$}
& \multicolumn{3}{c}{$k{=}3$}
& \multicolumn{3}{c}{$k{=}5$}
& \multicolumn{3}{c}{$k{=}10$} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\cmidrule(lr){11-13}
& Prec & Rec & NDCG
& Prec & Rec & NDCG
& Prec & Rec & NDCG
& Prec & Rec & NDCG \\
\midrule

CASE w/o CNN
& 0.3333 & 0.0740 & 0.3333
& 0.2666 & 0.1793 & 0.3046
& 0.2382 & 0.2519 & 0.3065
& 0.2023 & 0.3784 & 0.3297 \\

CASE w/o Set Encoder
& 0.5232 & 0.1263 & 0.5232
& 0.4485 & 0.2897 & 0.5038
& 0.3947 & 0.3915 & 0.5000
& 0.3122 & 0.5396 & 0.5152 \\

CASE w/o Item Embedding
& 0.5390 & 0.1281 & 0.5390
& 0.4488 & 0.2848 & 0.5057
& 0.3919 & 0.3847 & 0.4985
& 0.3124 & 0.5360 & 0.5161 \\

CASE w/ PermEqMean
& \underline{0.5414} & \underline{0.1326} & \underline{0.5414}
& \underline{0.4538} & \underline{0.2901} & \underline{0.5115}
& \underline{0.3983} & \underline{0.3917} & \underline{0.5062}
& \underline{0.3147} & \underline{0.5463} & \underline{0.5225} \\

CASE (w/ ISAB)
& \textbf{0.5478} & \textbf{0.1325} & \textbf{0.5478}
& \textbf{0.4551} & \textbf{0.2901} & \textbf{0.5135}
& \textbf{0.3989} & \textbf{0.3949} & \textbf{0.5086}
& \textbf{0.3155} & \textbf{0.5464} & \textbf{0.5241} \\

\bottomrule
\end{tabular}
\end{table*}

% \begin{table*}[t]
% \centering
% \small
% \caption{Ablation study on the Instacart dataset. Higher is better. Best results are in \textbf{bold}; second best are \underline{underlined}.}
% \begin{tabular}{l|ccc|ccc|ccc|ccc}
% \toprule
% \multirow{2}{*}{Model Components}
% & \multicolumn{3}{c|}{$K{=}1$}
% & \multicolumn{3}{c|}{$K{=}3$}
% & \multicolumn{3}{c|}{$K{=}5$}
% & \multicolumn{3}{c}{$K{=}10$} \\
% & Prec & Rec & NDCG
%   & Prec & Rec & NDCG
%   & Prec & Rec & NDCG
%   & Prec & Rec & NDCG \\
% \midrule

% CASE w/o CNN
% & 0.3333 & 0.0740 & 0.3333
% & 0.2666 & 0.1793 & 0.3046
% & 0.2382 & 0.2519 & 0.3065
% & 0.2023 & 0.3784 & 0.3297 \\
% % & 0.1655 & 0.5319 & 0.3760 \\

% CASE w/o Set Encoder
% & 0.5232 & 0.1263 & 0.5232
% & 0.4485 & 0.2897 & 0.5038
% & 0.3947 & 0.3915 & 0.5000
% & 0.3122 & 0.5396 & 0.5152 \\
% % & 0.2342 & 0.6978 & 0.5582 \\

% CASE w/o Item Embedding
% & 0.5390 & 0.1281 & 0.5390
% & 0.4488 & 0.2848 & 0.5057
% & 0.3919 & 0.3847 & 0.4985
% & 0.3124 & 0.5360 & 0.5161 \\
% % & 0.2336 & 0.6913 & 0.5588 \\

% CASE w/ PermEqMean
% & \underline{0.5414} & \underline{0.1326} & \underline{0.5414}
% & \underline{0.4538} & \underline{0.2901} & \underline{0.5115}
% & \underline{0.3983} & \underline{0.3917} & \underline{0.5062}
% & \underline{0.3147} & \underline{0.5463} & \underline{0.5225}\\
% % & \underline{0.2356} & \underline{0.6927} & \underline{0.5656} \\

% CASE (w/ ISAB)
% & \textbf{0.5478} & \textbf{0.1325} & \textbf{0.5478}
% & \textbf{0.4551} & \textbf{0.2901} & \textbf{0.5135}
% & \textbf{0.3989} & \textbf{0.3949} & \textbf{0.5086}
% & \textbf{0.3155} & \textbf{0.5464} & \textbf{0.5241} \\
% % & \textbf{0.2359} & \textbf{0.6994} & \textbf{0.5660} \\

% \bottomrule
% \label{tab:ablation}
% \end{tabular}
% \end{table*}


Table~\ref{tab:main_results} compares CASE against four baselines across four datasets. We assess the model along two axes: offline recommendation quality and scalability to production.

% \begin{table}[t]
% \centering
% \small
% \caption{Dataset statistics.}
% \begin{tabular}{lrrrrr}
% \toprule
% Dataset & \#U & \#I & Avg\_Size\_B & Avg\_B/U & \#B \\
% \midrule
% DC & 123,935 & 852 & 1.60 & 14.16 & 1,754,890 \\
% Instacart & 18,739 & 37,522 & 10.07 & 16.70 & 312,986 \\
% Ta-Feng & 7,227 & 18,703 & 6.58 & 7.52 & 54,342 \\
% Retail-Prop & 10,308 & 88,812 & 11.72 & 43.00 & 443,232 \\
% \bottomrule
% \end{tabular}
% \label{tab:dataset_stats}
% \end{table}
% TIFUKNN leads at $K{=}1$ while CASE outperforms at $K{\geq}3$ and lareger, the practically more relevant regime given that customers routinely place large baskets. On the Proprietary dataset, 

CASE achieves the best or near-best performance across all datasets and metrics. The primary offline competitor is TIFUKNN, a well-established strong baseline for repurchase recommendation. On TaFeng (sparse histories) and DC (small basket context), CASE clearly leads, with PIETSP ranking second, confirming that calendar-time cadence is a stronger design principle in sparse settings: when transactions are few or baskets are small, neighbor aggregation degrades, whereas CASE’s shared temporal encoding generalizes across the population. On richer datasets (Instacart and Proprietary), CASE remains competitive with TIFUKNN and slightly outperforms it, which is a strong result given TIFUKNN’s effectiveness as a repurchase baseline~\cite{li2023next}. DNNTSP and BERT4NBR consistently lag across DC, Instacart, and Proprietary, indicating that basket-index encoding is a structural limitation that more complex graph or transformer architectures do not overcome.
Beside recommendation quality, CASE also keepsparameterization independent of the user population. For a user with $n$ candidate items and a $T$-day horizon, multi-scale temporal encoding and induced set attention incur $O(n(T+K))$ complexity. TIFUKNN, by contrast, requires $O(|\mathcal{U}| \cdot d)$ per-query computation to retrieve and aggregate neighbor histories, which is infeasible at the scale of tens of millions of users. \YC{What are $K, d, U$?}
% In addition, CASE maintains strong performance in the absence of item embeddings, a property that is particularly advantageous for production deployment, as we demonstrate in the next section.

Overall, CASE is the only approach in our comparison that is simultaneously competitive with the strongest offline baseline and deployable at production scale.

\subsection{Ablation Study}

The ablation study is evaluated on the Instacart dataset and isolates the contribution of each architectural component, shown in Table~\ref{tab:ablation}.

\textbf{Temporal CNN is the dominant component.} Removing the multi-scale CNN leads to the largest performance drop across all metrics, confirming that calendar-time cadence encoding provides the primary discriminative signal. 
Figure~\ref{fig:embedding} supports these findings. The temporal CNN induces a clear separation between positive and negative items by organizing them according to cadence phase in the embedding space. The subsequent set encoder (ISAB) preserves this global structure while refining the decision boundary through cross-item co-purchase context, resulting in more compact and better-separated clusters.
% As shown in Figure~\ref{fig:embedding}, the temporal CNN separates positives and negatives by aligning items according to cadence phase, while the set encoder further refines this separation by incorporating co-purchase context among items with similar cadence patterns.


\begin{figure}[h]
    \centering    
    \includegraphics[width=0.8\linewidth]{figure2.jpg}
    \caption{
    PCA of item embeddings (50 positive/negative samples each). Left: after temporal CNN; Right: after ISAB. CNN drives cadence-based separation and ISAB further refines it with cross-item context.
    }
    \label{fig:embedding}
\end{figure}

\textbf{Set Attention provides consistent gains across $K$.}
Removing the set encoder reduces performance at every cutoff. This suggests that ISAB captures co-purchase context that complements the temporal cadence signal throughout the item list.

\textbf{Item Embedding plays a complementary role.}
Removing item embeddings degrades performance modestly, indicating that semantic identity provides additional signal beyond cadence. The small gap confirms that calendar-time encoding is the primary driver of performance. This is significant in production: it reduces reliance on large, frequently refreshed embedding tables, improving scalability as the catalog grows.


% \textbf{ISAB vs.\ PermEqMean}. 
% We additionally compare ISAB to a lighter-weight permutation-equivariant mean pooling set encoder (PermEqMean)\cite{zaheer2017deep} to isolate whether attention-based set interaction is necessary beyond permutation-equivariant aggregation. CASE with ISAB maintains a consistent advantage across all cutoffs. This suggests that the learned induced-point attention in ISAB captures richer cross-item dependencies.


\textbf{ISAB vs.\ PermEqMean}. 
We futher compare ISAB to a lighter-weight permutation-equivariant mean pooling encoder (PermEqMean)~\cite{zaheer2017deep} to assess whether attention-based interaction provides benefits beyond simple set aggregation. PermEqMean achieves competitive performance, indicating that set-level encoding atop temporal cadence modeling is effective. ISAB further yields consistent gains, suggesting that induced-point attention refines cross-item dependencies beyond simple order invariant aggregation.


\section{Production Experimental Results}

We train the model at our production data scale and compare it against the deployed production system. In production, we focus on $k{\in}\{5, 10, 20\}$, as these represent the practically relevant recommendation slate sizes shown to users. As shown in Table~\ref{tab:lift}, CASE delivers sustained gains of 6.8--8.6\% in Precision and 7.9--9.9\% in Recall, with consistent improvements in NDCG. 

\begin{table}[h]
\centering
\small
\caption{Relative lift (\%) of CASE over production model.}
\begin{tabular}{c|ccc}
\toprule
k & Precision & Recall & NDCG \\
\midrule
5  & +8.63\%  & +9.90\%  & +10.46\% \\
10 & +6.78\%  & +7.95\%  & +9.40\% \\
20 & +5.27\%  & +6.32\%  & +8.75\% \\
\bottomrule
\end{tabular}
\label{tab:lift}
\end{table}


% Unlike neighbor-based approaches requiring per-user retrieval or graph expansion, 
CASE uses shared multi-scale temporal filters and induced set attention, ensuring that inference cost scales linearly with candidate size and remains independent of total user population. This design satisfies production constraints on scalability and infrastructure without introducing additional latency or per-user parameterization. An online A/B test is planned to evaluate CASE against the currently deployed model.

% \YC{We presented CASE, a strong and scalable next basket repurchase recommendation model, with consistent improvements over strong baselines across public benchmarks confirming that calendar-time cadence understanding, combined with scalable set-level attention, is a effective foundation for large-scale next basket repurchase recommendation}

% \section{Conclusion \YC{maybe remove conclusion}}

% We presented CASE, a strong and scalable next basket repurchase recommendation model that encodes item-level repurchase cadence via shared multi-scale convolutional filters and models cross-item dependencies through induced set attention. CASE achieves consistent improvements over strong baselines across three public benchmarks and delivers up to strong lift over a production system serving tens of millions of users. These results confirm that calendar-time cadence understanding, combined with scalable set-level attention, is a effective foundation for large-scale next basket repurchase recommendation.

\section*{Main Presenter Bio}

\noindent \textbf{Yanan Cao} 
is a Senior Data Scientist at Walmart Global Tech, working on large-scale retail recommendation systems. Her work focuses on next basket recommendation, purchase interval prediction, and temporal modeling for production-scale recommender systems using transformer-based deep learning and large language model reasoning.Her interests lie in scalable learning systems, and translating research innovations into real-world deployment.

\noindent \textbf{Ashish Ranjan} 
is a Staff Data Scientist at Walmart Global Tech, leading large-scale personalization and ranking systems. His work spans heterogeneous-set modeling, unified ranking across diverse content, and integration of large language models into production e-commerce workflows. He has led end-to-end deployments, from modeling to real-time inference infrastructure, serving hundreds of millions of users. His interests include scalable IR systems and the deployment of learning architectures at industrial scale.

% \clearpage
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
