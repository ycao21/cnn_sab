%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

% \documentclass[sigconf]{acmart}
\documentclass[sigconf,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}

% \documentclass[sigconf]{acmart}

\copyrightyear{2026}
\acmYear{2026}
\setcopyright{cc}
\setcctype{by}
\acmConference[SIGIR '26]{Proceedings of the ACM Web Conference 2026}{April 13--17, 2026}{Dubai, United Arab Emirates}
% \acmBooktitle{Proceedings of}
\acmPrice{}
\acmDOI{xxxxxxxxxxx}
\acmISBN{xxxxxxxxxxxx}

% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.



\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}

% % COMMENTS
\newif\ifcomment
\commenttrue % comment out to hide comments
\providecommand{\YC}[1]{\ifcomment{\small \color{blue} [YC: #1]}\fi} %Yanan
\providecommand{\AR}[1]{\ifcomment{\small \color{red} [AR: #1]}\fi} %Ashish
% \providecommand{\KZ}[1]{\ifcomment{\small \color{teal} [KZ: #1]}\fi} %
% \providecommand{\FF}[1]{\ifcomment{\small \color{olive} [FF: #1]}\fi} %
% \providecommand{\LMo}[1]{\ifcomment{\small \color{orange} [LMo: #1]}\fi} %
% \providecommand{\MD}[1]{\ifcomment{\small \color{cyan} [MD: #1]}\fi} %


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{CASE: Cadence-Aware Set Encoding for Large-Scale Next Basket Repurchase Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Anonymous Author(s)}
% \date{}
% \settopmatter{authorsperrow=4}

\author{Yanan Cao}
\authornote{Highlighted authors contributed equally to this research.}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{yanan.cao@walmart.com}

\author{Ashish Ranjan}
\authornotemark[1]
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ashish.ranjan0@walmart.com}

\author{Sinduja Subramaniam}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{sinduja.subramaniam@walmart.com}

\author{Evren Korpeoglu}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ekorpeoglu@walmart.com}

\author{Kaushiki Nag}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kaushiki.nag@walmart.com}

\author{Kannan Achan}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kannan.achan@walmart.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Author et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Repurchase prediction is the dominant signal in large-scale retail recommendation: most items in a user's next basket have been purchased before, and their timing follows a stable personal cadence. Yet most next basket recommendation (NBR) models frame the problem as a sequence of discrete basket events indexed by visit count, which cannot represent elapsed calendar time or adapt scores between transactions. Models that do capture calendar-time cadence rely on per-user hypernetworks to generate personalized filters, incurring parameterization costs that prohibit deployment at scale. We present CASE (Cadence-Aware Set Encoding), which decouples item-level temporal cadence encoding from cross-item set modeling. CASE applies shared multi-scale convolutional filters over a fixed calendar horizon to capture recurring repurchase rhythms, and uses induced set attention to model cross-item dependencies --- without per-user parameters. Evaluated against a deployed production system on over 10 million users and 500K items, CASE achieves up to 14\% relative Recall lift at $K{=}1$ and 8\% at $K{=}10$, demonstrating that scalable cadence-aware modeling yields measurable gains in industrial NBR.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Temporal reasoning}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Cadence Modeling, Next Basket Recommendation, Sequential Modeling}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\linewidth]{figure1.png}
    \caption{diagram}
    \label{fig:diagram}
\end{figure*}

In large-scale grocery and retail platforms, the majority of items in a user’s next basket have been purchased before. Repurchase timing follows item-specific cadence --- staples weekly, household goods monthly --- and accurate timing directly drives customer experience: recommend too early and suggestions feel irrelevant; too late and the item may already be in the cart from a competing channel. This makes next basket repurchase recommendation (NBRR) a central problem in production NBR systems, where the goal is to predict which previously purchased items a user needs next, and when.

Most neural NBRR methods model user history as an ordered sequence of basket \emph{events}, where time is represented implicitly by basket index rather than calendar days~\cite{yu2020predicting,ranjan2025scalable}. Under this formulation, a user with baskets on days 1, 8, and 36 is treated identically to one with baskets on days 1, 2, and 3 --- both appear as a 3-basket sequence with no distinction in elapsed intervals. Predictions are updated only when a new transaction is observed, meaning scores remain static between purchases and cannot reflect that an item is overdue or not yet due for repurchase. This is a fundamental mismatch with production deployment: a model that cannot reason about elapsed time cannot be meaningfully refreshed between user sessions, and it provides no signal for users whose purchase frequency varies over time.

Recent work showed that modeling purchase history as a calendar-time signal --- representing each item’s history as a binary time series over a fixed horizon --- and learning per-item repurchase cycles via convolution substantially improves NBRR~\cite{katz2024personalized}. This approach allows the model to detect, for example, that a user purchases milk every 7 days but paper towels every 28 days, and to update repurchase scores as calendar time advances without waiting for a new transaction. However, the model generates personalized convolutional filters for every user--item pair via a hypernetwork (making its parameters grow as $O(|\mathcal{U}| \times |\mathcal{I}|)$) and refines scores through full pairwise item attention ($O(N^2)$ in user history size). Both components are infeasible at production scale: generating new filter weights at inference time for tens of millions of users with histories of hundreds of items results in prohibitive latency and memory cost.

We propose \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding), which preserves the calendar-time cadence insight while remaining production-scalable. CASE uses \emph{shared} multi-scale convolutional filters --- spanning weekly, biweekly, monthly, seasonal, and trend horizons --- to encode recurring repurchase patterns without per-user parameterization. Cross-item dependencies within a user’s purchase history are captured through induced set attention blocks (ISAB)~\cite{lee2019set}, which reduce O($N^2$) pairwise attention to O($N{\cdot}K$) via a small set of learned induced points. Our contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=2pt]
  \item We identify the basket-index formulation as a fundamental limitation of existing NBRR for production deployment, and motivate calendar-time cadence as the appropriate inductive bias.
  \item We propose CASE, combining shared multi-scale CNN cadence encoding with induced set attention, requiring no per-user parameters and supporting efficient batch inference.
  \item We demonstrate up to 14\% relative Recall lift over a deployed production system on 10M+ users and 500K items, and consistent gains across three public benchmarks.
\end{itemize}

\section{CASE: Model Architecture}
\label{sec:model}

\textbf{Problem Formulation.}
Let $\mathcal{B}_u = (B_1, B_2, \ldots, B_L)$ denote the ordered sequence of baskets for user $u$, where each $B_l \subseteq \mathcal{I}$ is a set of items purchased on a specific calendar date $d_l$. The NBRR task is to rank items in the user's purchase history $\mathcal{I}_u = \bigcup_{l} B_l$ by their likelihood of appearing in the next basket $B_{L+1}$. We focus on the repurchase candidate set $\mathcal{I}_u$ because in grocery and retail settings, the vast majority of next-basket items are drawn from a user's prior purchase history; new-item discovery is a complementary but separate problem~\cite{cao2025s2srec2}. The model architecture (illustrated in Figure~\ref{fig:diagram}) processes all items in $\mathcal{I}_u$ jointly to produce per-item repurchase scores.

\textbf{Calendar-Time History Representation.}
For each item $i \in \mathcal{I}_u$, we construct a binary purchase indicator vector $\mathbf{h}_i \in \{0,1\}^T$ over a rolling window of $T{=}364$ calendar days ending at prediction time, where $h_{i,t} = 1$ if item $i$ was purchased on day $t$ (zero-indexed from the start of the window). This representation is fundamentally different from basket-index encoding: it preserves the actual temporal spacing between purchases, allowing the model to distinguish a user who purchases milk every 7 days from one who purchases it every 28 days, and to capture seasonal effects such as higher repurchase frequency of barbecue products in summer. Because the window is aligned to calendar time, the same encoding can be used at any point in time without retraining, enabling live score updates between transactions. Each item is also assigned a dense learned embedding $\mathbf{e}_i \in \mathbb{R}^{d_e}$ from a shared embedding table of size $|\mathcal{I}| \times d_e$, which captures semantic item identity independently of purchase timing.

\textbf{Multi-Scale Temporal CNN.}
To extract repurchase rhythm features from $\mathbf{h}_i$, we apply a shared multi-scale 1D convolutional network with five kernel sizes corresponding to natural retail repurchase cycles: weekly ($k{=}7$), biweekly ($k{=}14$), monthly ($k{=}28$), seasonal ($k{=}91$), and trend ($k{=}182$) days. For each scale $k$, a single-channel Conv1d layer uses stride $k$ (non-overlapping windows), producing $\lfloor T/k \rfloor$ activations that summarize purchase activity within each non-overlapping time window of length $k$. Specifically, the $j$-th activation at scale $k$ aggregates purchase activity in days $[jk, (j+1)k)$. The outputs from all five scales are concatenated, giving a vector of $\sum_k \lfloor T/k \rfloor = 52 + 26 + 13 + 4 + 2 = 97$ features per item, which is then passed through two fully connected layers with ReLU activation to produce a $d_c$-dimensional temporal feature $\mathbf{c}_i = \text{CNN}(\mathbf{h}_i) \in \mathbb{R}^{d_c}$. The CNN weights are shared across all users and items --- there are no per-user or per-item parameters in the temporal encoder --- making the parameter count independent of the number of users. The combined item representation fed to the set encoder is $\mathbf{x}_i = [\mathbf{e}_i \,\|\, \mathbf{c}_i] \in \mathbb{R}^{d_e + d_c}$, concatenating semantic identity with temporal cadence features.

\textbf{Induced Set Attention Encoding.}
Items in $\mathcal{I}_u$ form an unordered set: the prediction task is symmetric with respect to item ordering, and co-purchase patterns within the set provide additional repurchase signals (e.g., items that are frequently purchased together tend to repurchase together). To model these dependencies in a permutation-equivariant manner, we apply a Set Transformer encoder~\cite{lee2019set} based on Induced Set Attention Blocks (ISAB). ISAB factorizes the full $O(N^2)$ pairwise attention across $N$ items into two multi-head attention (MAB) operations via $K$ shared learnable \emph{induced points} $\mathbf{I} \in \mathbb{R}^{K \times d_h}$: first the induced points attend to the item set ($\mathbf{H} = \text{MAB}(\mathbf{I}, \mathbf{X})$, complexity $O(K \cdot N)$), then items attend back to the induced points ($\text{MAB}(\mathbf{X}, \mathbf{H})$, complexity $O(N \cdot K)$), for total complexity $O(N \cdot K)$ rather than $O(N^2)$. We stack two ISAB layers with $K{=}32$ induced points and $H{=}4$ attention heads (layer normalization enabled), where the first layer maps from $d_e + d_c$ to $d_h$ and the second preserves $d_h$. The output is a set of enriched item representations $\{\mathbf{z}_i\}_{i \in \mathcal{I}_u}$ with $\mathbf{z}_i \in \mathbb{R}^{d_h}$, where each $\mathbf{z}_i$ reflects both the item's own temporal cadence pattern and its co-purchase context within the user's history.

\textbf{Scoring and Training.}
Each enriched item representation $\mathbf{z}_i$ is passed through a two-layer MLP scorer with hidden dimension $d_h/2$, ELU activation, and dropout, to produce a scalar repurchase score $s_i = \text{MLP}(\mathbf{z}_i) \in \mathbb{R}$. At inference, all items in $\mathcal{I}_u$ are scored in a single forward pass and ranked by $s_i$ to produce the top-$K$ recommendation list. For training, we use binary cross-entropy loss over all items in $\mathcal{I}_u$: given a basket transition from $B_l$ to $B_{l+1}$, item $i$ is labeled positive ($y_i = 1$) if $i \in B_{l+1} \cap \mathcal{I}_u$ and negative ($y_i = 0$) otherwise. This directly optimizes the precision of repurchase ranking within the user's history. Since all parameters --- item embeddings, CNN filters, ISAB induced points, and the MLP scorer --- are shared across users, CASE adds no per-user parameters and its memory footprint is independent of the user population size, supporting deployment at scales of tens of millions of users.

\section{Experimental Setting}

\subsection{Datasets}

We evaluate on four datasets spanning grocery and retail domains (Table~\ref{tab:dataset_stats}), chosen to cover a range of catalog sizes, basket densities, and user scales.

\textbf{Instacart}~\cite{yasserh_instacart_2017} is a large public online grocery dataset with 18,739 users, 37,522 products, and an average of 10 baskets per user at 16.7 items each. It covers diverse categories (fresh produce, dairy, household goods) and is the most widely used public benchmark for next basket research. \textbf{TaFeng}~\cite{yu2020predicting} is a Taiwanese cash-and-carry supermarket dataset with 7,227 users and 18,703 items; its shorter histories (6.58 baskets/user) and smaller baskets (7.52 items) make it a challenging setting for cadence models. \textbf{DC} is derived from the Dunnhumby ``The Complete Journey'' loyalty-card records\footnote{\url{https://www.dunnhumby.com/source-files/}} and contains 123,935 users but only 852 distinct products, with very sparse histories (1.60 baskets/user); it tests models under a closed, small item catalog. \textbf{Proprietary} is our internal large-scale grocery dataset with 10,308 users across 88,812 items --- more than twice the Instacart catalog --- and rich histories (11.72 baskets, 43 items/basket on average). This dataset is the primary industrial benchmark and the basis for the production lift experiment.

For all datasets, we apply leave-one-out evaluation: each user's last basket is the test target; all preceding baskets form the training set. Item relevance is defined as the intersection of the test basket with the user's training history, i.e., we measure ranking quality over repurchase candidates only. Calendar timestamps are preserved as absolute dates to construct the 364-day binary history window at test time.

\begin{table}[h]
\centering
\small
\caption{Dataset statistics.}
\begin{tabular}{lrrrrr}
\toprule
Dataset & \#Users & \#Items & Avg.Bskt & Avg.Size & \#Baskets \\
\midrule
Instacart & 18,739 & 37,522 & 10.07 & 16.70 & 312,986 \\
TaFeng & 7,227 & 18,703 & 6.58 & 7.52 & 54,342 \\
DC & 123,935 & 852 & 1.60 & 14.16 & 1,754,890 \\
Proprietary & 10,308 & 88,812 & 11.72 & 43.00 & 443,232 \\
\bottomrule
\end{tabular}
\label{tab:dataset_stats}
\end{table}

\subsection{Baselines}

We compare against three baselines. \textbf{PersonalTop} is a non-parametric baseline that ranks each user’s historical items by their global purchase frequency within the training set. Despite its simplicity, it is a strong baseline in the repurchase setting because items purchased frequently in the past are likely to be purchased again. \textbf{DNNTSP}~\cite{yu2020predicting} is a deep set-based model for next basket prediction that constructs a weighted item co-occurrence graph over the user’s basket sequence and uses GNN aggregation to capture inter-item relationships, followed by a temporal attention mechanism over basket embeddings. It represents basket history using basket-index timestamps without calendar time. \textbf{PIETSP}~\cite{ranjan2025scalable} is a more recent scalable model that extends temporal set prediction with permutation-equivariant architectures, improving efficiency and sequence modeling quality over DNNTSP. Like DNNTSP, it treats time as basket index rather than calendar days and therefore cannot model the inter-purchase intervals that drive cadence-aware repurchase prediction.

\subsection{Evaluation and Implementation}

We report Precision@$K$, Recall@$K$, and NDCG@$K$ for $K \in \{1, 3, 5, 10\}$. Precision@$K$ measures the fraction of recommended items that are relevant; Recall@$K$ measures the fraction of all relevant items captured in the top-$K$ list; NDCG@$K$ is a ranking-aware metric that assigns higher scores to relevant items appearing earlier in the list. In the NBRR setting, a relevant item at rank 1 is more valuable than one at rank 10 since it appears at the top of the recommendation list shown to the user.

CASE is implemented in PyTorch with item embedding dimension $d_e{=}256$, CNN output dimension $d_c{=}128$, ISAB hidden dimension $d_h{=}256$, $K{=}32$ induced points, and $H{=}4$ attention heads. We train for 50 epochs using the Adam optimizer with learning rate $10^{-3}$, weight decay $10^{-5}$, and batch size 64. Dropout of 0.1 is applied after each ISAB layer and in the MLP scorer. All models are trained on a single GPU; no dataset-specific hyperparameter tuning is performed --- the same configuration is used across all four datasets.

\section{Experimental Results}

% Style like your screenshot: dataset blocks + grouped K headers.
% Requires: \usepackage{booktabs,multirow}
% Optional: \usepackage{array} for better column spacing
\begin{table*}[t]
\centering
\small
\caption{Comparisons with different methods on Top-$K$ performance (repurchase next-basket recommendation). Higher is better. Best results within each dataset and metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{@{}ll|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}3$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c}{$K{=}10$} \\
& & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

% ========================= Instacart =========================
\multirow{4}{*}{Instacart}
& PersonalTop
& \underline{0.5019} & \underline{0.1173} & \underline{0.5019}
& \underline{0.4139} & \underline{0.2593} & \underline{0.4670}
& \underline{0.3574} & 0.3458 & \underline{0.4556}
& 0.2819 & 0.4821 & \textbf{0.4671} \\
& DNNTSP
& 0.4631 & 0.1025 & 0.4631
& 0.4002 & 0.2511 & 0.4447
& 0.3527 & \underline{0.3480} & 0.4427
& \underline{0.2842} & \underline{0.4943} & 0.4616 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.5339} & \textbf{0.1282} & \textbf{0.5339}
& \textbf{0.4508} & \textbf{0.2896} & \textbf{0.5072}
& \textbf{0.3971} & \textbf{0.3937} & \textbf{0.5040}
& \textbf{0.3118} & \textbf{0.5397} & \underline{0.5170} \\
\midrule

% ========================= Ta-Feng =========================
\multirow{4}{*}{TaFeng}
& PersonalTop
& 0.1771 & 0.0988 & 0.1771
& 0.1363 & 0.2100 & 0.2042
& 0.1104 & 0.2776 & 0.2263
& 0.0795 & 0.3839 & 0.2612 \\
& DNNTSP
& \underline{0.2387} & \underline{0.1632} & \underline{0.2387}
& 0.1675 & 0.3196 & 0.2869
& 0.1406 & 0.4255 & 0.3275
& 0.1086 & 0.5809 & 0.3739 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.2606} & \textbf{0.1708} & \textbf{0.2606}
& \textbf{0.1832} & \textbf{0.3306} & \textbf{0.3071}
& \textbf{0.1519} & \textbf{0.4318} & \textbf{0.3441}
& \textbf{0.1131} & \textbf{0.5932} & \textbf{0.3902} \\
\midrule

% ========================= DC =========================
\multirow{4}{*}{DC}
& PersonalTop
& \underline{0.3434} & \underline{0.2879} & \underline{0.3434}
& 0.2233 & 0.5424 & 0.4489
& 0.1672 & 0.6559 & 0.4828
& 0.1059 & 0.7838 & \underline{0.5000} \\
& DNNTSP
& 0.3264 & 0.2685 & 0.3264
& 0.2269 & 0.5485 & 0.4441
& 0.1710 & 0.6696 & 0.4790
& 0.1085 & 0.8057 & 0.4931 \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.3906} & \textbf{0.3300} & \textbf{0.3906}
& \textbf{0.2512} & \textbf{0.6120} & \textbf{0.5094}
& \textbf{0.1845} & \textbf{0.7236} & \textbf{0.5407}
& \textbf{0.1139} & \textbf{0.8446} & \textbf{0.5479} \\
\midrule

% ========================= Proprietary =========================
\multirow{4}{*}{Proprietary}
& PersonalTop
& \underline{0.3686} & \underline{0.0940} & \underline{0.3686}
& 0.2744 & 0.1691 & 0.3248
& 0.2314 & 0.2206 & 0.3151
& 0.1772 & 0.2981 & 0.3126 \\
& DNNTSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& PIETSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE (Ours)
& \textbf{0.3849} & \textbf{0.1004} & \textbf{0.3849}
& \textbf{--} & \textbf{--} & \textbf{--} % TODO: K=3 not in ablation, needs re-run
& \textbf{0.2593} & \textbf{0.2535} & \textbf{0.3495}
& \textbf{0.2009} & \textbf{0.3438} & \textbf{0.3500} \\
\bottomrule
\end{tabular}
\label{tab:topk_grouped_results}
\end{table*}

<<<<<<< HEAD
Table~\ref{tab:topk_grouped_results} summarizes results across all four datasets. CASE consistently outperforms all baselines on every public dataset across all $K$ values, with the sole exception of NDCG@10 on Instacart where it still achieves 0.5170 vs.\ the best baseline's 0.4671 --- a 10.7\% relative gain. The margins are largest at $K{=}1$, where precision-focused ranking is most demanding: on Instacart, CASE's Precision@1 of 0.534 improves over PersonalTop's 0.502 by 6.4\%; on DC, CASE achieves 0.391 vs.\ PersonalTop's 0.343 (+14.0\%); on TaFeng, CASE's 0.261 compares favorably to DNNTSP's 0.239 (+9.2\%).

Across datasets, PersonalTop is consistently the strongest non-CASE model, frequently outperforming DNNTSP at low $K$. This reflects the strong frequency prior in repurchase behavior: items that a user has purchased many times are likely to be needed again. However, PersonalTop applies a static global ranking and cannot account for whether an item was purchased recently (making it unlikely to be repurchased immediately) or long ago (making it overdue). CASE's calendar-time encoding captures exactly this signal --- Precision@1 is highest precisely at the cut where cadence timing is most discriminative.

DNNTSP's basket-index architecture underperforms PersonalTop on Instacart and DC at $K{=}1$, suggesting that learned co-occurrence patterns, without temporal cadence, do not improve over frequency-based ranking for precision-focused repurchase. DNNTSP does close the gap at larger $K$ (e.g., its Recall@10 of 0.494 on Instacart slightly exceeds PersonalTop's 0.482), indicating that item co-occurrence captures useful signal at coarser granularity. CASE outperforms on both dimensions, combining cadence precision with set-level co-purchase context.

On the Proprietary dataset, CASE outperforms PersonalTop at all measured $K$ values. Against the deployed production system, CASE delivers a 14.1\% relative lift in Precision@1 and 14.6\% in Recall@1, with sustained gains of 6.8--8.6\% across $K{=}5$ and $K{=}10$ (Table~\ref{tab:lift}).
=======
\subsection{Main Results}

Table~\ref{tab:topk_grouped_results} shows results across all datasets. CASE outperforms all baselines on every public dataset at every $K$. The gains are most pronounced at $K{=}1$: on Instacart CASE improves Precision@1 from 0.502 (PersonalTop) to 0.534 (+6.4\%), and on DC from 0.343 to 0.391 (+14.0\%). PersonalTop ranks second in most settings, reflecting the strong frequency signal in repurchase behavior; however, it cannot distinguish items that are due for repurchase from those that are not, whereas CASE's calendar-time encoding captures precisely this timing distinction. DNNTSP, despite using learned item co-occurrence, underperforms PersonalTop at low $K$ on Instacart and DC, where cadence timing is decisive. On the Proprietary dataset, CASE consistently outperforms PersonalTop across all $K$ values, and delivers a 14\% relative Precision and Recall lift over the deployed production system at $K{=}1$ (Table~\ref{tab:lift}).
>>>>>>> origin/overleaf-2026-02-24-0744

\subsection{Ablation Study}

\begin{table*}[t]
\centering
\small
\caption{Ablation study on the Proprietary dataset. Higher is better. Best results per metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Model Components}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c|}{$K{=}10$}
& \multicolumn{3}{c}{$K{=}20$} \\
& Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

CNN w/ Set Attention
& \underline{0.3849} & \underline{0.1004} & \underline{0.3849}
& \underline{0.2593} & \textbf{0.2535} & \textbf{0.3495}
& \textbf{0.2009} & \textbf{0.3438} & \textbf{0.3500}
& \textbf{0.1488} & \textbf{0.4645} & \textbf{0.3714} \\

  + Set pooling
& 0.3724 & 0.0991 & 0.3724
& 0.2563 & 0.2492 & 0.3450
& \underline{0.1980} & \underline{0.3423} & \underline{0.3460}
& \underline{0.1445} & \underline{0.4539} & \underline{0.3648} \\


CNN w/ Set Perm Mean
& \textbf{0.3897} & \textbf{0.1038} & \textbf{0.3897}
& \textbf{0.2597} & \underline{0.2505} & \underline{0.3489}
& 0.1974 & 0.3353 & 0.3459
& 0.1433 & 0.4426 & 0.3626 \\

% CNN w/ Set attention Encoder plus pooling
% & 0.3724 & 0.0991 & 0.3724
% & 0.2563 & 0.2492 & 0.3450
% & \underline{0.1980} & \underline{0.3423} & \underline{0.3460}
% & \underline{0.1445} & \underline{0.4539} & \underline{0.3648} \\

CNN w/o any set encoder
& 0.3767 & 0.0984 & 0.3767
& 0.2525 & 0.2455 & 0.3399
& 0.1974 & 0.3406 & 0.3434
& 0.1435 & 0.4440 & 0.3598 \\

% No\_SetTran\_PMA
% & 0.3772 & 0.0993 & 0.3772
% & 0.2571 & 0.2493 & 0.3450
% & 0.1977 & 0.3408 & 0.3449
% & 0.1463 & \underline{0.4567} & \underline{0.3661} \\

Set Attention w/o CNN
& 0.2226 & 0.0628 & 0.2226
& 0.1292 & 0.1454 & 0.1872
& 0.1020 & 0.2092 & 0.1921
& 0.0754 & 0.2912 & 0.2064 \\

\bottomrule
\end{tabular}
\end{table*}


\begin{table}[t]
\centering
\small
\caption{Relative lift (\%) of the proposed model over the production model.}
\begin{tabular}{c|ccc}
\toprule
K & Precision & Recall & NDCG \\
\midrule
1  & +14.11\% & +14.62\% & +14.11\% \\
5  & +8.63\%  & +9.90\%  & +10.46\% \\
10 & +6.78\%  & +7.95\%  & +9.40\% \\
20 & +5.27\%  & +6.32\%  & +8.75\% \\
\bottomrule
\end{tabular}
\label{tab:lift}
\end{table}

The ablation study (Table~\ref{tab:topk_grouped_results} and the ablation table, evaluated on the Proprietary dataset) isolates the contribution of each architectural component.

\textbf{Temporal CNN is the dominant component.} Removing the multi-scale CNN (``Set Attention w/o CNN'') causes the largest performance drop across all metrics: Precision@1 collapses from 0.385 to 0.223 ($-$42\%), Recall@5 from 0.254 to 0.145 ($-$43\%), and NDCG@10 from 0.350 to 0.192 ($-$45\%). This confirms that the calendar-time cadence encoding is the primary source of discriminative signal, and no amount of set-level attention can compensate for its absence.

\textbf{ISAB provides consistent gains, especially at larger $K$.} Removing the set encoder (``CNN w/o any set encoder'') reduces performance across all $K$ values, with the effect growing at $K{=}10$ and $K{=}20$: Recall@10 drops from 0.344 to 0.341 and Recall@20 from 0.465 to 0.444. This suggests that ISAB captures co-purchase context that becomes more important as we expand the recommendation list, since at larger $K$ the temporal cadence signal alone may not distinguish between multiple items at similar phases in their repurchase cycle.

\textbf{Dual scoring does not help.} Adding a global compatibility score via PMA pooling (``+ Set pooling'') slightly reduces performance relative to intrinsic-only scoring, indicating that the per-item scores derived from ISAB representations are sufficient. The global pooling introduces a mixing of information across items that appears to add noise rather than signal in this setting.

\textbf{ISAB vs.\ PermEqMean.} Both set encoder variants (ISAB and permutation-equivariant mean pooling, ``CNN w/ Set Perm Mean'') perform comparably at $K{=}1$, with PermEqMean slightly ahead (0.390 vs.\ 0.385 in Precision@1). However, ISAB maintains a consistent advantage at $K{\geq}5$, achieving better Recall@10 (0.344 vs.\ 0.335) and NDCG@10 (0.350 vs.\ 0.346). This suggests that the learned induced-point attention in ISAB captures richer cross-item dependencies that benefit broader recommendation lists.

\section{Conclusion}

We presented CASE, a cadence-aware set encoding model for large-scale next basket repurchase recommendation. By representing each item's purchase history as a binary calendar-time indicator and applying shared multi-scale convolutional filters, CASE captures item-level repurchase rhythms without per-user parameters. Induced set attention then enriches item representations with cross-item co-purchase context at sub-quadratic cost. CASE achieves consistent improvements over strong baselines across three public benchmarks and delivers up to 14\% relative Recall and Precision lift over a deployed production system serving millions of users. These results demonstrate that decoupling cadence encoding from user-specific parameterization is both practically necessary and empirically effective for production-scale NBRR. Future work includes integrating CASE with complementary basket completion models~\cite{cao2025s2srec2} for a unified next-basket system.



% \clearpage
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
