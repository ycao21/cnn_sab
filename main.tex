%%

%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

% \documentclass[sigconf]{acmart}
\documentclass[sigconf,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}

% \documentclass[sigconf]{acmart}

\copyrightyear{2026}
\acmYear{2026}
\setcopyright{cc}
\setcctype{by}
\acmConference[SIGIR '26]{Proceedings of the ACM Web Conference 2026}{April 13--17, 2026}{Dubai, United Arab Emirates}
% \acmBooktitle{Proceedings of}
\acmPrice{}
\acmDOI{xxxxxxxxxxx}
\acmISBN{xxxxxxxxxxxx}

% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.



\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}

% % COMMENTS
\newif\ifcomment
\commenttrue % comment out to hide comments
\providecommand{\YC}[1]{\ifcomment{\small \color{blue} [YC: #1]}\fi} %Yanan
\providecommand{\AR}[1]{\ifcomment{\small \color{red} [AR: #1]}\fi} %Ashish
% \providecommand{\KZ}[1]{\ifcomment{\small \color{teal} [KZ: #1]}\fi} %
% \providecommand{\FF}[1]{\ifcomment{\small \color{olive} [FF: #1]}\fi} %
% \providecommand{\LMo}[1]{\ifcomment{\small \color{orange} [LMo: #1]}\fi} %
% \providecommand{\MD}[1]{\ifcomment{\small \color{cyan} [MD: #1]}\fi} %


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{CASE: Cadence-Aware Set Encoding for Large-Scale Next Basket Repurchase Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Anonymous Author(s)}
% \date{}
% \settopmatter{authorsperrow=4}

\author{Yanan Cao}
\authornote{Highlighted authors contributed equally to this research.}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{yanan.cao@walmart.com}

\author{Ashish Ranjan}
\authornotemark[1]
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ashish.ranjan0@walmart.com}

\author{Sinduja Subramaniam}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{sinduja.subramaniam@walmart.com}

\author{Evren Korpeoglu}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{ekorpeoglu@walmart.com}

\author{Kaushiki Nag}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kaushiki.nag@walmart.com}

\author{Kannan Achan}
\affiliation{%
  \institution{Walmart Global Tech}
  \city{Sunnyvale}
  \state{CA}
  \country{USA}
}
\email{kannan.achan@walmart.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Author et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Repurchase behavior is a major driver of large-scale retail recommendation: most items in a user’s next basket have been purchased before, and their timing follows a stable personal cadence. Yet most next basket recommendation (NBR) models frame the problem as a sequence of discrete basket events indexed by visit count, which cannot represent elapsed calendar time or adapt scores between transactions. Models that capture purchase cadence often rely on per-user hypernetworks to generate personalized filters, incurring parameterization costs that hinder scalability. We present \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding for NBR), which decouples item-level temporal cadence encoding from cross-item set modeling. CASE applies shared multi-scale convolutional filters over a fixed calendar horizon to capture recurring repurchase rhythms, and uses induced set attention to model cross-item dependencies without user-specific parameterization. Across four public benchmarks and a proprietary dataset, CASE consistently outperforms strong NBR baselines across Precision, Recall, and NDCG at multiple cutoffs. Evaluated against a deployed production system on over 10 million users and 500K items, CASE achieves up to 8.6\% relative Precision lift at $K{=}5$ and 6.8\% at $K{=}10$, demonstrating that scalable cadence-aware modeling yields measurable gains in both benchmark and industrial settings.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Temporal reasoning}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Next Basket Recommendation, Sequential Modeling, Temporal Prediction}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\linewidth]{figure1.png}
    \caption{diagram (\YC{will update this})}
    \label{fig:diagram}
\end{figure*}

In large-scale grocery and retail platforms, the majority of items in a user’s next basket have been purchased before. Repurchase timing follows item-specific cadence, such as staples weekly and household goods monthly. Thus, accurate timing directly drives customer experience: recommend too early and suggestions feel irrelevant; too late and the item may already be in the cart from a competing channel. This makes next basket repurchase recommendation (NBRR) a central problem in production NBR systems, where the goal is to predict which previously purchased items a user needs next, and when \YC{do we need and when? seems not address in the model}.

Most neural NBRR methods model user history as an ordered sequence of basket \emph{events}, where time is represented implicitly by basket index rather than cadence around calendar days in real-world ~\cite{yu2020predicting,ranjan2025scalable}. Under this formulation, a user with baskets on days 1, 8, and 36 is treated identically to one with baskets on days 1, 2, and 3, since they both appear as a 3-basket sequence with no distinction in elapsed intervals. Predictions are updated only when a new transaction is observed, meaning scores remain static between purchases and cannot reflect that an item is overdue or not yet due for repurchase. This is a fundamental mismatch with production deployment: a model that cannot reason about elapsed time cannot be meaningfully refreshed between user sessions, and it provides no signal for users whose purchase frequency varies over time.

An alternative line of work uses KNN-based methods that model personalized item frequency with temporal decay, finding similar users and aggregating their purchase patterns~\cite{hu2020modeling}. While competitive on offline benchmarks, such methods require computing user-to-user similarities at inference time --- $O(|\mathcal{U}|)$ per query --- which is infeasible at the scale of tens of millions of users without approximate retrieval systems that introduce their own quality and latency tradeoffs.

A separate recent work showed that modeling purchase history as a calendar-time signal by representing each item’s history as a binary time series over a fixed horizon, and learning per-item repurchase cycles via convolution substantially improves NBRR~\cite{katz2024personalized}. This approach captures that a user purchases milk every 7 days but paper towels every 28 days, and can update repurchase scores as calendar time advances. However, it generates personalized convolutional filters for every user--item pair via a hypernetwork and refines scores through full pairwise item attention ($O(N^2)$ in history size) --- both quadratic components that are infeasible at production scale.

We propose \textbf{CASE} (\textbf{C}adence-\textbf{A}ware \textbf{S}et \textbf{E}ncoding), which preserves the calendar-time cadence insight while remaining production-scalable. CASE uses shared multi-scale convolutional filters, including weekly, biweekly, monthly, seasonal, and trend horizons to encode recurring repurchase patterns. Cross-item dependencies within a user’s purchase history are captured through induced set attention blocks (ISAB)~\cite{lee2019set}, which reduce O($N^2$) pairwise attention to O($N{\cdot}K$) via a small set of learned induced points. Our contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=2pt]
  \item We identify the basket-index formulation as a fundamental limitation of existing NBRR for production deployment, and motivate calendar-time cadence as the appropriate inductive bias.
  \item We propose CASE, combining shared multi-scale CNN cadence encoding with induced set attention, requiring no per-user parameters and supporting efficient batch inference.
  \item We demonstrate up to 14\% relative Precision and Recall lift over a deployed production system on 10M+ users and 500K items, and consistent gains across three public benchmarks.
\end{itemize}

\section{CASE: Model Architecture}
\label{sec:model}

\textbf{Problem Formulation.}
Let $\mathcal{B}_u = (B_1, B_2, \ldots, B_L)$ denote the ordered basket sequence for user $u$, where $B_l \subseteq \mathcal{I}$ is purchased on calendar date $d_l$. The NBRR task ranks items in the repurchase history $\mathcal{I}_u = \bigcup_l B_l$ by their likelihood of appearing in $B_{L+1}$. The model (Figure~\ref{fig:diagram}) processes all items in $\mathcal{I}_u$ jointly to produce per-item scores.

\textbf{Calendar-Time History Representation.}
For each item $i \in \mathcal{I}_u$, we construct a binary purchase indicator $\mathbf{h}_i \in \{0,1\}^T$ over a rolling $T$-day calendar window, where $h_{i,t}{=}1$ if item $i$ was purchased on day $t$. Unlike basket-index encoding, this preserves actual inter-purchase intervals, enabling the model to distinguish items with different repurchase cadences and capture seasonal effects. Each item also has a learned embedding $\mathbf{e}_i \in \mathbb{R}^{d_e}$ capturing semantic identity.

\textbf{Multi-Scale Temporal CNN.}
Shared multi-scale Conv1d filters are applied over $\mathbf{h}_i$ at five kernel sizes — weekly ($k{=}7$), biweekly ($k{=}14$), monthly ($k{=}28$), seasonal ($k{=}91$), and trend ($k{=}182$) — each with stride $k$ (non-overlapping windows). The $\lfloor T/k \rfloor$ activations per scale are concatenated across all scales and projected through two FC layers with ReLU to yield $\mathbf{c}_i \in \mathbb{R}^{d_c}$. All CNN weights are shared across users and items. The combined representation is $\mathbf{x}_i = [\mathbf{e}_i \,\|\, \mathbf{c}_i]$.

\textbf{Induced Set Attention Encoding.}
Items in $\mathcal{I}_u$ form an unordered set; co-purchase patterns provide additional repurchase signal. We model cross-item dependencies with a Set Transformer encoder~\cite{lee2019set} using Induced Set Attention Blocks (ISAB), which reduce $O(N^2)$ pairwise attention to $O(N{\cdot}K)$ via $K$ learnable induced points $\mathbf{I} \in \mathbb{R}^{K \times d_h}$: induced points first attend to the item set ($\mathbf{H} = \text{MAB}(\mathbf{I}, \mathbf{X})$), then items attend back ($\text{MAB}(\mathbf{X}, \mathbf{H})$). Two ISAB layers with $K{=}32$ and $H{=}4$ heads produce enriched representations $\mathbf{z}_i \in \mathbb{R}^{d_h}$.

\textbf{Scoring and Training.}
Each $\mathbf{z}_i$ is passed through a two-layer MLP to produce a scalar score $s_i$; items are ranked by $s_i$ at inference. Training uses binary cross-entropy: items in $B_{l+1} \cap \mathcal{I}_u$ are positive, all others negative. All parameters — embeddings, CNN, ISAB, and MLP — are shared across users, making CASE's memory footprint independent of user population size.

\section{Experimental Setting}

\subsection{Datasets}

We evaluate on four datasets spanning grocery and retail domains, chosen to cover a range of catalog sizes, basket densities, and user scales.

\textbf{Instacart}~\cite{yasserh_instacart_2017} is a large public online grocery dataset with 18,739 users, 37,522 products, and an average of 10 baskets per user at 16.7 items each. It covers diverse categories (fresh produce, dairy, household goods) and is the most widely used public benchmark for next basket research. \textbf{TaFeng}~\cite{yu2020predicting} is a Taiwanese cash-and-carry supermarket dataset with 7,227 users and 18,703 items; its shorter histories (6.58 baskets/user) and smaller baskets (7.52 items) make it a challenging setting for cadence models. \textbf{DC} is derived from the Dunnhumby ``Carbo-Loading'' database\footnote{\url{https://www.dunnhumby.com/source-files/}} and contains 123,935 users but only 852 distinct products, with very sparse histories (1.60 baskets/user); it tests models under a closed, small item catalog. \textbf{Proprietary} is randomly sampled from our internal large-scale grocery dataset, having 10,308 users across a large amount of 88,812 items, and rich histories (11.72 baskets, 43 items/basket on average). 

For all datasets, we apply leave-one-out evaluation: each user's last repurchasee item basket is the test target; all preceding baskets form the training set. Item candidate is defined as the intersection of the test basket with the user's training history. Due to some dataset don't provide dates, calendar timestamps are preserved as relevant days between orders, to construct the n-day binary history window at test time.

% \begin{table}[h]
% \centering
% \small
% \caption{Dataset statistics.}
% \begin{tabular}{lrrrrr}
% \toprule
% Dataset & \#Users & \#Items & Avg.Bskt & Avg.Size & \#Baskets \\
% \midrule
% Instacart & 18,739 & 37,522 & 10.07 & 16.70 & 312,986 \\
% TaFeng & 7,227 & 18,703 & 6.58 & 7.52 & 54,342 \\
% DC & 123,935 & 852 & 1.60 & 14.16 & 1,754,890 \\
% Proprietary & 10,308 & 88,812 & 11.72 & 43.00 & 443,232 \\
% \bottomrule
% \end{tabular}
% \label{tab:dataset_stats}
% \end{table}

\subsection{Baselines}

We compare CASE against four baseline models. \textbf{PersonalTop} is a non-parametric baseline that ranks each user’s historical items by their personal purchase frequency. Despite its simplicity, it is a strong baseline in the repurchase setting since items purchased frequently in the past are likely to be purchased again. \textbf{TIFUKNN}~\cite{hu2020modeling} is a KNN-based method that models personalized item frequency with temporal decay: it assigns exponentially decayed weights to past purchase events and aggregates scores from a user’s own history and their nearest neighbors in user space. TIFUKNN is competitive on offline benchmarks but fundamentally non-scalable in production: for each query user, it must compute similarities against all other users and retrieve their histories, resulting in $O(|\mathcal{U}| \cdot N)$ inference cost. At tens of millions of users this is infeasible without approximate nearest neighbor approximations that degrade recommendation quality, and the method has no straightforward batching or online serving path. \textbf{DNNTSP}~\cite{yu2020predicting} is a deep set-based model for next basket prediction that constructs a weighted item co-occurrence graph over the user’s basket sequence and uses GNN aggregation to capture inter-item relationships, followed by a temporal attention mechanism over basket embeddings. \textbf{PIETSP}~\cite{ranjan2025scalable} is a more recent scalable model that extends temporal set prediction with permutation-equivariant architectures, improving efficiency and sequence modeling quality over DNNTSP. Both DNNTSP and PIETSP treat time as basket index rather than calendar days.

\subsection{Evaluation and Implementation}

We report Precision@$K$, Recall@$K$, and NDCG@$K$ for $K \in \{1, 3, 5, 10\}$. Precision@$K$ measures the fraction of recommended items that are relevant, which is the primary metrics in real-world recommendation production system; Recall@$K$ measures the fraction of all relevant items captured in the top-$K$ list; NDCG@$K$ is a ranking-aware metric that assigns higher scores to relevant items appearing earlier in the list. In the NBRR production setting, a relevant item at rank 1 is more valuable than one at rank 10 since it appears at the top of the recommendation list shown to the user.

CASE is implemented in PyTorch with item embedding dimension $d_e{=}128$, CNN output dimension $d_c{=}128$, ISAB hidden dimension $d_h{=}256$, $K{=}32$ induced points, and $H{=}4$ attention heads. We train for 30 epochs using the Adam optimizer with learning rate $10^{-3}$, weight decay $10^{-5}$, and batch size 64. Dropout of 0.1 is applied after each ISAB layer and in the MLP scorer. All models are trained on a single GPU.

\section{Experimental Results}

% Style like your screenshot: dataset blocks + grouped K headers.
% Requires: \usepackage{booktabs,multirow}
% Optional: \usepackage{array} for better column spacing
\begin{table*}[t]
\centering
\small
\caption{Comparisons on Top-$K$ performance for next-basket repurchase recommendation. Higher is better. Best results per dataset and metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{@{}ll|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}3$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c}{$K{=}10$} \\
& & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

% ========================= Instacart =========================
\multirow{4}{*}{Instacart}
& Personal\_Top
& 0.5019 & 0.1173 & 0.5019
& 0.4139 & 0.2593 & 0.4670
& 0.3574 & 0.3458 & 0.4556
& 0.2819 & 0.4821 & 0.4671 \\
& TIFUKNN
& \textbf{0.5489} & \textbf{0.1340} & \textbf{0.5489}
& \textbf{0.4541} & \underline{0.2880} & \textbf{0.5131}
& \underline{0.3930} & \underline{0.3878} & \underline{0.5037}
& \underline{0.3115} & \underline{0.5371} & \textbf{0.5189} \\
& DNNTSP
& 0.4631 & 0.1025 & 0.4631
& 0.4002 & 0.2511 & 0.4447
& 0.3527 & 0.3480 & 0.4427
& 0.2842 & 0.4943 & 0.4616 \\
& CASE
& \underline{0.5339} & \underline{0.1282} & \underline{0.5339}
& \underline{0.4508} & \textbf{0.2896} & \underline{0.5072}
& \textbf{0.3971} & \textbf{0.3937} & \textbf{0.5040}
& \textbf{0.3118} & \textbf{0.5397} & \underline{0.5170} \\
\midrule

% ========================= Ta-Feng =========================
\multirow{4}{*}{Ta-Feng}
& Personal\_Top
& 0.1771 & 0.0988 & 0.1771
& 0.1363 & 0.2100 & 0.2042
& 0.1104 & 0.2776 & 0.2263
& 0.0795 & 0.3839 & 0.2612 \\
& TIFUKNN
& 0.2146 & 0.1201 & 0.2146
& 0.1639 & 0.2646 & 0.2503
& 0.1332 & 0.3443 & 0.2769
& 0.0995 & 0.5007 & 0.3228 \\
& DNNTSP
& \underline{0.2387} & \underline{0.1632} & \underline{0.2387}
& \underline{0.1675} & \underline{0.3196} & \underline{0.2869}
& \underline{0.1406} & \underline{0.4255} & \underline{0.3275}
& \underline{0.1086} & \underline{0.5809} & \underline{0.3739} \\
& CASE
& \textbf{0.2606} & \textbf{0.1708} & \textbf{0.2606}
& \textbf{0.1832} & \textbf{0.3306} & \textbf{0.3071}
& \textbf{0.1519} & \textbf{0.4318} & \textbf{0.3441}
& \textbf{0.1131} & \textbf{0.5932} & \textbf{0.3902} \\
\midrule

% ========================= DC =========================
\multirow{4}{*}{DC}
& Personal\_Top
& 0.3434 & 0.2879 & 0.3434
& 0.2233 & 0.5424 & 0.4489
& 0.1672 & 0.6559 & 0.4828
& 0.1059 & 0.7838 & 0.5000 \\
& TIFUKNN
& \underline{0.3843} & \underline{0.3245} & \underline{0.3843}
& \underline{0.2498} & \underline{0.6081} & \underline{0.5051}
& \underline{0.1843} & \underline{0.7229} & \underline{0.5387}
& \textbf{0.1140} & \underline{0.8442} & \underline{0.5473} \\
& DNNTSP
& 0.3264 & 0.2685 & 0.3264
& 0.2269 & 0.5485 & 0.4441
& 0.1710 & 0.6696 & 0.4790
& 0.1085 & 0.8057 & 0.4931 \\
& CASE
& \textbf{0.3906} & \textbf{0.3300} & \textbf{0.3906}
& \textbf{0.2512} & \textbf{0.6120} & \textbf{0.5094}
& \textbf{0.1845} & \textbf{0.7236} & \textbf{0.5407}
& \underline{0.1139} & \textbf{0.8446} & \textbf{0.5479} \\
\midrule

% ========================= Walmart =========================
\multirow{4}{*}{Walmart}
& Personal\_Top
& 0.3672 & 0.0921 & 0.3672
& 0.2744 & 0.1694 & 0.3251
& 0.2310 & 0.2205 & 0.3152
& 0.1766 & 0.2962 & 0.3121 \\
& TIFUKNN
& \underline{0.3856} & \underline{0.0971} & \underline{0.3856}
& \underline{0.3010} & \underline{0.1913} & \underline{0.3534}
& \underline{0.2605} & \textbf{0.2550} & \underline{0.3503}
& \textbf{0.2038} & \textbf{0.3547} & \textbf{0.3543} \\
& DNNTSP
& -- & -- & --
& -- & -- & --
& -- & -- & --
& -- & -- & -- \\
& CASE
& \textbf{0.3871} & \textbf{0.1007} & \textbf{0.3871}
& \textbf{0.3058} & \textbf{0.1930} & \textbf{0.3587}
& \textbf{0.2611} & \underline{0.2540} & \textbf{0.3509}
& \underline{0.2017} & \underline{0.3448} & \underline{0.3514} \\
\bottomrule
\end{tabular}
\label{tab:main_results}
\end{table*}

% \begin{table*}[t]
% \centering
% \small
% \caption{Comparisons with different methods on Top-$K$ performance (repurchase next-basket recommendation). Higher is better. Best results within each dataset and metric@K are in \textbf{bold}; second best are \underline{underlined}.}
% \begin{tabular}{@{}ll|ccc|ccc|ccc|ccc@{}}
% \toprule
% \multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods}
% & \multicolumn{3}{c|}{$K{=}1$}
% & \multicolumn{3}{c|}{$K{=}3$}
% & \multicolumn{3}{c|}{$K{=}5$}
% & \multicolumn{3}{c}{$K{=}10$} \\
% & & Prec & Rec & NDCG
%   & Prec & Rec & NDCG
%   & Prec & Rec & NDCG
%   & Prec & Rec & NDCG \\
% \midrule

% % ========================= Instacart =========================
% \multirow{4}{*}{Instacart}
% & PersonalTop
% & \underline{0.5019} & \underline{0.1173} & \underline{0.5019}
% & \underline{0.4139} & \underline{0.2593} & \underline{0.4670}
% & \underline{0.3574} & 0.3458 & \underline{0.4556}
% & 0.2819 & 0.4821 & \textbf{0.4671} \\
% & DNNTSP
% & 0.4631 & 0.1025 & 0.4631
% & 0.4002 & 0.2511 & 0.4447
% & 0.3527 & \underline{0.3480} & 0.4427
% & \underline{0.2842} & \underline{0.4943} & 0.4616 \\
% & PIETSP
% & -- & -- & --
% & -- & -- & --
% & -- & -- & --
% & -- & -- & -- \\
% & CASE (Ours)
% & \textbf{0.5339} & \textbf{0.1282} & \textbf{0.5339}
% & \textbf{0.4508} & \textbf{0.2896} & \textbf{0.5072}
% & \textbf{0.3971} & \textbf{0.3937} & \textbf{0.5040}
% & \textbf{0.3118} & \textbf{0.5397} & \underline{0.5170} \\
% \midrule

% % ========================= Ta-Feng =========================
% \multirow{4}{*}{TaFeng}
% & PersonalTop
% & 0.1771 & 0.0988 & 0.1771
% & 0.1363 & 0.2100 & 0.2042
% & 0.1104 & 0.2776 & 0.2263
% & 0.0795 & 0.3839 & 0.2612 \\
% & DNNTSP
% & \underline{0.2387} & \underline{0.1632} & \underline{0.2387}
% & 0.1675 & 0.3196 & 0.2869
% & 0.1406 & 0.4255 & 0.3275
% & 0.1086 & 0.5809 & 0.3739 \\
% & PIETSP
% & -- & -- & --
% & -- & -- & --
% & -- & -- & --
% & -- & -- & -- \\
% & CASE (Ours)
% & \textbf{0.2606} & \textbf{0.1708} & \textbf{0.2606}
% & \textbf{0.1832} & \textbf{0.3306} & \textbf{0.3071}
% & \textbf{0.1519} & \textbf{0.4318} & \textbf{0.3441}
% & \textbf{0.1131} & \textbf{0.5932} & \textbf{0.3902} \\
% \midrule

% % ========================= DC =========================
% \multirow{4}{*}{DC}
% & PersonalTop
% & \underline{0.3434} & \underline{0.2879} & \underline{0.3434}
% & 0.2233 & 0.5424 & 0.4489
% & 0.1672 & 0.6559 & 0.4828
% & 0.1059 & 0.7838 & \underline{0.5000} \\
% & DNNTSP
% & 0.3264 & 0.2685 & 0.3264
% & 0.2269 & 0.5485 & 0.4441
% & 0.1710 & 0.6696 & 0.4790
% & 0.1085 & 0.8057 & 0.4931 \\
% & PIETSP
% & -- & -- & --
% & -- & -- & --
% & -- & -- & --
% & -- & -- & -- \\
% & CASE (Ours)
% & \textbf{0.3906} & \textbf{0.3300} & \textbf{0.3906}
% & \textbf{0.2512} & \textbf{0.6120} & \textbf{0.5094}
% & \textbf{0.1845} & \textbf{0.7236} & \textbf{0.5407}
% & \textbf{0.1139} & \textbf{0.8446} & \textbf{0.5479} \\
% \midrule

% % ========================= Proprietary =========================
% \multirow{4}{*}{Proprietary}
% & PersonalTop
% & \underline{0.3686} & \underline{0.0940} & \underline{0.3686}
% & 0.2744 & 0.1691 & 0.3248
% & 0.2314 & 0.2206 & 0.3151
% & 0.1772 & 0.2981 & 0.3126 \\
% & DNNTSP
% & -- & -- & --
% & -- & -- & --
% & -- & -- & --
% & -- & -- & -- \\
% & PIETSP
% & -- & -- & --
% & -- & -- & --
% & -- & -- & --
% & -- & -- & -- \\
% & CASE (Ours)
% & \textbf{0.3849} & \textbf{0.1004} & \textbf{0.3849}
% & \textbf{0.3047} & \textbf{0.1922} & \textbf{0.3572} % TODO: K=3 not in ablation, needs re-run
% & \textbf{0.2593} & \textbf{0.2535} & \textbf{0.3495}
% & \textbf{0.2009} & \textbf{0.3438} & \textbf{0.3500} \\
% \bottomrule
% \end{tabular}
% \label{tab:topk_grouped_results}
% \end{table*}

Table~\ref{tab:main_results} compares CASE against all baselines across four datasets.

\textbf{TaFeng and DC.} CASE achieves the best or near-best results across almost every metric and $K$ on both datasets. Both have sparse per-user histories (6.58 and 1.60 baskets/user respectively), which undermines TIFUKNN's neighbor estimation: with few transactions, user similarity is unreliable and neighbor aggregation degrades. CASE is robust in this regime because the shared CNN is trained on the full user population and encodes population-level repurchase rhythms --- even a user with 6 baskets benefits from these shared temporal patterns.

\textbf{Instacart.} TIFUKNN leads at $K{=}1$ but CASE outperforms across $K{\geq}5$. Given that Instacart's average basket size is 16.7 items, customers routinely place large orders, making higher-$K$ recommendations the more operationally relevant regime. At $K{=}1$ neighbor consensus on the single most-purchased item is decisive; at larger $K$, models must rank many items with similar long-run frequencies, and CASE's cadence encoding --- distinguishing items that are ``overdue'' from those recently purchased --- provides the finer-grained signal that TIFUKNN's frequency aggregation cannot.

\textbf{Walmart.} CASE and TIFUKNN are closely matched, with CASE slightly ahead at $K{\leq}5$ and TIFUKNN marginally leading at $K{=}10$. TIFUKNN is a well-established strong baseline for next basket repurchase recommendation, and CASE being competitive with it is itself a meaningful result --- with the critical distinction that CASE requires no per-user parameters and runs in a single shared forward pass, while TIFUKNN's $O(|\mathcal{U}|{\cdot}N)$ inference cost is infeasible at production scale. DNNTSP's basket-index representation underperforms PersonalTop at low $K$ across all datasets, confirming that discarding calendar time is a concrete structural limitation.

\subsection{Ablation Study}

The ablation study (evaluated on the Walmart dataset) isolates the contribution of each architectural component.

\textbf{Temporal CNN is the dominant component.} Removing the multi-scale CNN causes the largest performance drop across all metrics: Precision@1 collapses from 0.385 to 0.223, Recall@5 from 0.254 to 0.145, and NDCG@10 from 0.350 to 0.192. This confirms that the calendar-time cadence encoding is the primary source of discriminative signal, and no amount of set-level attention can compensate for its absence.

\textbf{ISAB provides consistent gains, especially at larger $K$.} Removing the set encoder reduces performance across all $K$ values, with the effect growing at $K{=}10$ and $K{=}20$: Recall@10 drops from 0.344 to 0.341 and Recall@20 from 0.465 to 0.444. This suggests that ISAB captures co-purchase context that becomes more important as we expand the recommendation list, since at larger $K$ the temporal cadence signal alone may not distinguish between multiple items at similar phases in their repurchase cycle.

% \textbf{Dual scoring does not help.} Adding a global compatibility score via PMA pooling (``+ Set pooling'') slightly reduces performance relative to intrinsic-only scoring, indicating that the per-item scores derived from ISAB representations are sufficient. The global pooling introduces a mixing of information across items that appears to add noise rather than signal in this setting.

\textbf{ISAB vs.\ PermEqMean.} Both set encoder variants perform comparably at $K{=}1$, with PermEqMean slightly ahead (0.390 vs.\ 0.385 in Precision@1). However, ISAB maintains a consistent advantage at $K{\geq}5$, achieving better Recall@10 (0.344 vs.\ 0.335) and NDCG@10 (0.350 vs.\ 0.346). This suggests that the learned induced-point attention in ISAB captures richer cross-item dependencies that benefit broader recommendation lists.

\section{Comparison with Production}

We train and inference the model based on the production data scale and compare it against the deployed production system. CASE delivers a 14.1\% relative lift in Precision@1 and 14.6\% in Recall@1, with sustained gains of 6.8--8.6\% across $K{=}5$ and $K{=}10$ (Table~\ref{tab:lift}).

\begin{table}[h]
\centering
\small
\caption{Relative lift (\%) of the proposed model over the production model.}
\begin{tabular}{c|ccc}
\toprule
K & Precision & Recall & NDCG \\
\midrule
1  & +14.11\% & +14.62\% & +14.11\% \\
5  & +8.63\%  & +9.90\%  & +10.46\% \\
10 & +6.78\%  & +7.95\%  & +9.40\% \\
20 & +5.27\%  & +6.32\%  & +8.75\% \\
\bottomrule
\end{tabular}
\label{tab:lift}
\end{table}



\section{Conclusion}

We presented CASE, a cadence-aware set encoding model for large-scale next basket repurchase recommendation. By representing each item's purchase history as a binary calendar-time indicator and applying shared multi-scale convolutional filters, CASE captures item-level repurchase rhythms without per-user parameters. Induced set attention then enriches item representations with cross-item co-purchase context at sub-quadratic cost. CASE achieves consistent improvements over strong baselines across three public benchmarks and delivers up to 14\% relative Recall and Precision lift over a deployed production system serving millions of users. These results demonstrate that decoupling cadence encoding from user-specific parameterization is both practically necessary and empirically effective for production-scale NBRR. Future work includes integrating CASE with complementary basket completion models~\cite{cao2025s2srec2} for a unified next-basket system.





\begin{table*}[t]
\centering
\small
\caption{Ablation study on the Proprietary dataset. Higher is better. Best results per metric@K are in \textbf{bold}; second best are \underline{underlined}.}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Model Components}
& \multicolumn{3}{c|}{$K{=}1$}
& \multicolumn{3}{c|}{$K{=}5$}
& \multicolumn{3}{c|}{$K{=}10$}
& \multicolumn{3}{c}{$K{=}20$} \\
& Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG
  & Prec & Rec & NDCG \\
\midrule

CNN w/ Set Attention
& \underline{0.3871} & \underline{0.1007} & \underline{0.3871}
& \textbf{0.2601} & \textbf{0.2540} & \textbf{0.3509}
& \textbf{0.2017} & \textbf{0.3448} & \textbf{0.3514}
& \textbf{0.1492} & \textbf{0.4655} & \textbf{0.3726} \\

CNN w/ Set Perm Mean
& \textbf{0.3897} & \textbf{0.1038} & \textbf{0.3897}
& \underline{0.2597} & \underline{0.2505} & \underline{0.3489}
& \underline{0.1974} & 0.3353 & \underline{0.3459}
& 0.1433 & \underline{0.4426} & \underline{0.3626} \\

% CNN w/ Set attention Encoder plus pooling
% & 0.3724 & 0.0991 & 0.3724
% & 0.2563 & 0.2492 & 0.3450
% & \underline{0.1980} & \underline{0.3423} & \underline{0.3460}
% & \underline{0.1445} & \underline{0.4539} & \underline{0.3648} \\

CNN w/o any set encoder
& 0.3767 & 0.0984 & 0.3767
& 0.2525 & 0.2455 & 0.3399
& 0.1974 & \underline{0.3406} & 0.3434
& \underline{0.1435} & 0.4440 & 0.3598 \\

% No\_SetTran\_PMA
% & 0.3772 & 0.0993 & 0.3772
% & 0.2571 & 0.2493 & 0.3450
% & 0.1977 & 0.3408 & 0.3449
% & 0.1463 & \underline{0.4567} & \underline{0.3661} \\

Set Attention w/o CNN
& 0.2226 & 0.0628 & 0.2226
& 0.1292 & 0.1454 & 0.1872
& 0.1020 & 0.2092 & 0.1921
& 0.0754 & 0.2912 & 0.2064 \\

\bottomrule
\end{tabular}
\end{table*}




% \clearpage
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
